"""
Ollama export and import provider.

Ollama is a local model server that makes it easy to run LLMs locally.
Export converts Enigma AI Engine models to GGUF format for Ollama.
Import pulls models from Ollama's library.
"""

import json
import logging
import os
import shutil
import subprocess
from pathlib import Path
from typing import Any, Optional


from .base import (
    ExportProvider,
    ExportResult,
    ExportStatus,
    ImportProvider,
    ImportResult,
    ImportStatus,
)

logger = logging.getLogger(__name__)

# Check for llama.cpp convert script or llama-cpp-python
HAVE_LLAMA_CPP = False
try:
    HAVE_LLAMA_CPP = True
except ImportError:
    pass  # Intentionally silent

# Check for Ollama CLI
HAVE_OLLAMA = False
try:
    result = subprocess.run(["ollama", "--version"], capture_output=True, text=True, timeout=5)
    HAVE_OLLAMA = result.returncode == 0
except Exception:
    pass  # Intentionally silent


class OllamaProvider(ExportProvider):
    """
    Export models to Ollama format (GGUF).
    
    Ollama is a local model server. This provider creates:
    1. GGUF model file (quantized for efficiency)
    2. Modelfile for Ollama configuration
    
    Requirements:
        pip install llama-cpp-python  # For GGUF conversion
        # Or have llama.cpp installed
    
    Usage:
        provider = OllamaProvider()
        result = provider.export(
            "my_model",
            output_dir="./ollama_model",
            quantization="q4_0"  # Quantization level
        )
        
        # Then import to Ollama:
        # ollama create my-model -f ./ollama_model/Modelfile
    """
    
    NAME = "ollama"
    DESCRIPTION = "Export to Ollama - run models locally with simple CLI"
    REQUIRES_AUTH = False
    AUTH_ENV_VAR = None
    SUPPORTED_FORMATS = ["gguf"]
    WEBSITE = "https://ollama.ai"
    
    QUANTIZATION_TYPES = {
        "f32": "Full precision (largest, slowest)",
        "f16": "Half precision (large, fast on GPU)",
        "q8_0": "8-bit quantization (good quality, smaller)",
        "q4_0": "4-bit quantization (smaller, fast)",
        "q4_1": "4-bit quantization variant",
        "q5_0": "5-bit quantization (balance)",
        "q5_1": "5-bit quantization variant",
        "q2_k": "2-bit quantization (smallest, lower quality)",
        "q3_k_s": "3-bit small",
        "q3_k_m": "3-bit medium",
        "q3_k_l": "3-bit large",
        "q4_k_s": "4-bit small (recommended)",
        "q4_k_m": "4-bit medium (recommended)",
        "q5_k_s": "5-bit small",
        "q5_k_m": "5-bit medium",
        "q6_k": "6-bit (high quality)",
    }
    
    def _create_modelfile(
        self,
        output_path: Path,
        model_name: str,
        config: dict[str, Any],
        metadata: dict[str, Any],
        gguf_filename: str
    ):
        """Create Ollama Modelfile."""
        description = metadata.get("description", f"Enigma AI Engine model: {model_name}")
        
        modelfile = f"""# Modelfile for {model_name}
# Generated by Enigma AI Engine
# Import with: ollama create {model_name} -f Modelfile

FROM ./{gguf_filename}

# Model parameters
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40
PARAMETER num_ctx {config.get("max_seq_len", 1024)}

# System prompt (customize as needed)
SYSTEM \"\"\"You are {model_name}, a helpful AI assistant created with Enigma AI Engine. You provide clear, accurate, and helpful responses.\"\"\"

# Template for chat format
TEMPLATE \"\"\"{{{{ if .System }}}}<|system|>
{{{{ .System }}}}</s>
{{{{ end }}}}{{{{ if .Prompt }}}}<|user|>
{{{{ .Prompt }}}}</s>
{{{{ end }}}}<|assistant|>
{{{{ .Response }}}}</s>
\"\"\"

# License
LICENSE \"\"\"MIT License - Model trained with Enigma AI Engine\"\"\"
"""
        with open(output_path / "Modelfile", "w") as f:
            f.write(modelfile)
    
    def _convert_to_gguf(
        self,
        model_path: Path,
        output_path: Path,
        config: dict[str, Any],
        quantization: str = "q4_k_m"
    ) -> Optional[str]:
        """
        Convert PyTorch model to GGUF format.
        
        This is a simplified conversion - full GGUF support requires
        llama.cpp's convert scripts or manual implementation.
        """
        # For now, create a placeholder and instructions
        # Full GGUF conversion is complex and requires llama.cpp tools
        
        gguf_filename = f"model-{quantization}.gguf"
        gguf_path = output_path / gguf_filename
        
        # Create conversion script
        convert_script = f"""#!/bin/bash
# GGUF Conversion Script for {model_path.name}
# 
# This model needs to be converted to GGUF format using llama.cpp
# 
# Steps:
# 1. Clone llama.cpp: git clone https://github.com/ggerganov/llama.cpp
# 2. Install requirements: pip install -r llama.cpp/requirements.txt
# 3. Convert to GGUF:
#    python llama.cpp/convert.py {model_path} --outtype f16 --outfile model-f16.gguf
# 4. Quantize (optional but recommended):
#    ./llama.cpp/quantize model-f16.gguf {gguf_filename} {quantization}
#
# Or use llama-cpp-python:
#    pip install llama-cpp-python
#    # Then use the Python API for conversion

echo "Please run the conversion steps above to create {gguf_filename}"
"""
        with open(output_path / "convert.sh", "w") as f:
            f.write(convert_script)
        os.chmod(output_path / "convert.sh", 0o755)
        
        # Also save the PyTorch weights for conversion
        weights_src = model_path / "weights.pth"
        if weights_src.exists():
            shutil.copy(weights_src, output_path / "pytorch_model.bin")
        
        # Save config in llama.cpp compatible format
        llama_config = {
            "architectures": ["ForgeForCausalLM"],
            "vocab_size": config.get("vocab_size", 8000),
            "hidden_size": config.get("dim", 512),
            "intermediate_size": config.get("hidden_dim", 2048),
            "num_hidden_layers": config.get("n_layers", 8),
            "num_attention_heads": config.get("n_heads", 8),
            "num_key_value_heads": config.get("n_kv_heads", 8),
            "max_position_embeddings": config.get("max_seq_len", 1024),
            "rms_norm_eps": 1e-6,
            "rope_theta": config.get("rope_theta", 10000.0),
            "torch_dtype": "float32",
            "model_type": "forge",
        }
        with open(output_path / "config.json", "w") as f:
            json.dump(llama_config, f, indent=2)
        
        return gguf_filename
    
    def export(
        self,
        model_name: str,
        output_dir: Optional[str] = None,
        quantization: str = "q4_k_m",
        create_ollama: bool = True,
        **kwargs
    ) -> ExportResult:
        """
        Export to Ollama/GGUF format.
        
        Args:
            model_name: Enigma AI Engine model name
            output_dir: Output directory
            quantization: GGUF quantization type (q4_k_m recommended)
            create_ollama: Also create in Ollama (requires ollama CLI)
        """
        try:
            model_path = self._get_model_path(model_name)
            
            if not output_dir:
                output_dir = str(self.models_dir / f"{model_name}_ollama")
            
            output_path = Path(output_dir)
            output_path.mkdir(parents=True, exist_ok=True)
            
            # Load model info
            config = self._load_config(model_path)
            metadata = self._load_metadata(model_path)
            
            # Convert to GGUF (creates placeholder + instructions)
            gguf_filename = self._convert_to_gguf(
                model_path, output_path, config, quantization
            )
            
            # Create Modelfile
            self._create_modelfile(
                output_path, model_name, config, metadata, gguf_filename
            )
            
            # Create README
            readme = f"""# {model_name} for Ollama

Enigma AI Engine model packaged for Ollama.

## Quick Start

### Step 1: Convert to GGUF
Run the conversion script or follow manual steps:
```bash
./convert.sh
```

### Step 2: Import to Ollama
```bash
cd {output_path}
ollama create {model_name} -f Modelfile
```

### Step 3: Run
```bash
ollama run {model_name}
```

## Quantization Options

Available quantization types:
{chr(10).join(f'- `{k}`: {v}' for k, v in self.QUANTIZATION_TYPES.items())}

Current: `{quantization}`

## Files

- `Modelfile` - Ollama configuration
- `config.json` - Model architecture
- `pytorch_model.bin` - Original weights (needs conversion)
- `convert.sh` - Conversion helper script
"""
            with open(output_path / "README.md", "w") as f:
                f.write(readme)
            
            # Try to create in Ollama if requested
            if create_ollama:
                try:
                    result = subprocess.run(
                        ["ollama", "list"],
                        capture_output=True,
                        text=True,
                        timeout=10
                    )
                    if result.returncode == 0:
                        return ExportResult(
                            status=ExportStatus.PARTIAL,
                            provider=self.NAME,
                            model_name=model_name,
                            local_path=str(output_path),
                            message=(
                                f"Ollama package created. Convert to GGUF then run:\n"
                                f"  cd {output_path} && ollama create {model_name} -f Modelfile"
                            )
                        )
                except FileNotFoundError:
                    pass  # Ollama not installed
            
            return ExportResult(
                status=ExportStatus.SUCCESS,
                provider=self.NAME,
                model_name=model_name,
                local_path=str(output_path),
                message=(
                    f"Ollama package created at {output_path}\n"
                    f"1. Convert weights to GGUF (see convert.sh)\n"
                    f"2. Run: ollama create {model_name} -f Modelfile"
                )
            )
            
        except Exception as e:
            logger.exception("Ollama export failed")
            return ExportResult(
                status=ExportStatus.FAILED,
                provider=self.NAME,
                model_name=model_name,
                message=str(e)
            )


class OllamaImporter(ImportProvider):
    """
    Import models from Ollama library.
    
    Ollama has a library of pre-built models you can pull and run locally.
    This importer downloads models and registers them in Enigma AI Engine.
    
    Usage:
        importer = OllamaImporter()
        
        # List available models
        models = importer.list_models()
        
        # Search
        results = importer.search("llama")
        
        # Import (pull) a model
        result = importer.import_model("llama2")
    """
    
    NAME = "ollama"
    DESCRIPTION = "Import from Ollama - pull and run models locally"
    REQUIRES_AUTH = False
    AUTH_ENV_VAR = None
    SUPPORTED_FORMATS = ["gguf"]
    WEBSITE = "https://ollama.ai"
    
    # Popular Ollama models
    POPULAR_MODELS = [
        {"id": "llama2", "name": "Llama 2", "size": "7B", "description": "Meta's Llama 2 base model"},
        {"id": "llama2:13b", "name": "Llama 2 13B", "size": "13B", "description": "Larger Llama 2"},
        {"id": "llama2:70b", "name": "Llama 2 70B", "size": "70B", "description": "Largest Llama 2"},
        {"id": "mistral", "name": "Mistral", "size": "7B", "description": "Mistral AI's 7B model"},
        {"id": "mixtral", "name": "Mixtral", "size": "8x7B", "description": "Mixture of experts model"},
        {"id": "codellama", "name": "Code Llama", "size": "7B", "description": "Code-specialized Llama"},
        {"id": "phi", "name": "Phi-2", "size": "2.7B", "description": "Microsoft's small but capable model"},
        {"id": "neural-chat", "name": "Neural Chat", "size": "7B", "description": "Intel's chat model"},
        {"id": "starling-lm", "name": "Starling", "size": "7B", "description": "Berkeley's RLHF model"},
        {"id": "vicuna", "name": "Vicuna", "size": "7B", "description": "Fine-tuned LLaMA"},
        {"id": "orca-mini", "name": "Orca Mini", "size": "3B", "description": "Small but capable"},
        {"id": "tinyllama", "name": "TinyLlama", "size": "1.1B", "description": "Very small Llama"},
    ]
    
    def search(
        self,
        query: str,
        limit: int = 10,
        **kwargs
    ) -> list[dict[str, Any]]:
        """Search for Ollama models."""
        query_lower = query.lower()
        
        results = [
            m for m in self.POPULAR_MODELS
            if query_lower in m["id"].lower() or 
               query_lower in m["name"].lower() or
               query_lower in m.get("description", "").lower()
        ]
        
        return results[:limit] if results else self.POPULAR_MODELS[:limit]
    
    def list_models(self, **kwargs) -> list[dict[str, Any]]:
        """List locally installed Ollama models."""
        if not HAVE_OLLAMA:
            return []
        
        try:
            result = subprocess.run(
                ["ollama", "list"],
                capture_output=True,
                text=True,
                timeout=10
            )
            
            if result.returncode != 0:
                return []
            
            # Parse output (NAME, ID, SIZE, MODIFIED)
            lines = result.stdout.strip().split("\n")[1:]  # Skip header
            models = []
            for line in lines:
                parts = line.split()
                if parts:
                    models.append({
                        "id": parts[0],
                        "name": parts[0].split(":")[0],
                        "installed": True,
                    })
            return models
            
        except Exception as e:
            logger.warning(f"Failed to list Ollama models: {e}")
            return []
    
    def import_model(
        self,
        source_id: str,
        local_name: Optional[str] = None,
        **kwargs
    ) -> ImportResult:
        """
        Pull an Ollama model and register it in Enigma AI Engine.
        
        Args:
            source_id: Ollama model name (e.g., "llama2", "mistral:7b")
            local_name: Local name in Enigma AI Engine registry
        """
        if not HAVE_OLLAMA:
            return ImportResult(
                status=ImportStatus.FAILED,
                provider=self.NAME,
                model_name=local_name or source_id,
                source_id=source_id,
                message="Ollama not installed. Install from: https://ollama.ai"
            )
        
        try:
            # Determine local name
            if not local_name:
                local_name = f"ollama_{source_id.replace(':', '_').replace('/', '_')}".lower()
            
            local_name = local_name.lower().strip().replace(" ", "_")
            model_path = self.models_dir / local_name
            
            if model_path.exists():
                return ImportResult(
                    status=ImportStatus.ALREADY_EXISTS,
                    provider=self.NAME,
                    model_name=local_name,
                    source_id=source_id,
                    local_path=str(model_path),
                    message=f"Model already exists at {model_path}"
                )
            
            # Pull the model using Ollama CLI
            logger.info(f"Pulling {source_id} from Ollama...")
            result = subprocess.run(
                ["ollama", "pull", source_id],
                capture_output=True,
                text=True,
                timeout=3600  # Model downloads can take a long time
            )
            
            if result.returncode != 0:
                return ImportResult(
                    status=ImportStatus.FAILED,
                    provider=self.NAME,
                    model_name=local_name,
                    source_id=source_id,
                    message=f"Failed to pull model: {result.stderr}"
                )
            
            # Create Enigma AI Engine wrapper directory
            model_path.mkdir(parents=True, exist_ok=True)
            
            # Create config pointing to Ollama
            config = {
                "source": "ollama",
                "model_id": source_id,
                "type": "ollama_model",
            }
            with open(model_path / "config.json", "w") as f:
                json.dump(config, f, indent=2)
            
            # Create wrapper script
            wrapper_code = f'''"""
Ollama Model Wrapper for {source_id}
Generated by Enigma AI Engine
"""

import subprocess
import json

MODEL_ID = "{source_id}"

def generate(prompt: str, **kwargs) -> str:
    """Generate text using Ollama."""
    result = subprocess.run(
        ["ollama", "run", MODEL_ID, prompt],
        capture_output=True,
        text=True
    )
    return result.stdout

def chat(message: str, system: str = "", **kwargs) -> str:
    """Chat interface."""
    return generate(message)

# For API access
def api_generate(prompt: str, **kwargs) -> str:
    """Use Ollama's API endpoint."""
    import requests
    response = requests.post(
        "http://localhost:11434/api/generate",
        json={{"model": MODEL_ID, "prompt": prompt, **kwargs}},
        timeout=120
    )
    return response.json().get("response", "")

if __name__ == "__main__":
    print(chat("Hello!"))
'''
            with open(model_path / "wrapper.py", "w") as f:
                f.write(wrapper_code)
            
            # Register
            self._register_model(
                local_name=local_name,
                model_path=model_path,
                source="ollama",
                source_id=source_id,
                metadata={"ollama_id": source_id, "type": "ollama_model"}
            )
            
            return ImportResult(
                status=ImportStatus.SUCCESS,
                provider=self.NAME,
                model_name=local_name,
                source_id=source_id,
                local_path=str(model_path),
                message=f"Pulled {source_id} from Ollama. Run with: ollama run {source_id}"
            )
            
        except Exception as e:
            logger.exception("Ollama import failed")
            return ImportResult(
                status=ImportStatus.FAILED,
                provider=self.NAME,
                model_name=local_name or source_id,
                source_id=source_id,
                message=str(e)
            )