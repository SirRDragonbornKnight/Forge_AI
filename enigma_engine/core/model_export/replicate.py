"""
Replicate export and import provider.

Replicate is a platform for running and sharing ML models via API.
"""

import json
import logging
import shutil
from pathlib import Path
from typing import Any, Optional

import torch

from .base import (
    ExportProvider,
    ExportResult,
    ExportStatus,
    ImportProvider,
    ImportResult,
    ImportStatus,
)

logger = logging.getLogger(__name__)

# Check for Replicate client
HAVE_REPLICATE = False
replicate_client = None
try:
    HAVE_REPLICATE = True
except ImportError:
    pass

# Check for Cog (Replicate's model packaging tool)
HAVE_COG = False
try:
    import subprocess
    result = subprocess.run(["cog", "--version"], capture_output=True, text=True, timeout=5)
    HAVE_COG = result.returncode == 0
except Exception:
    pass


class ReplicateProvider(ExportProvider):
    """
    Export models to Replicate.
    
    Replicate lets you run models in the cloud with a simple API.
    Your model gets its own API endpoint that anyone can call.
    
    Requirements:
        pip install replicate
        # Install Cog for building: https://github.com/replicate/cog
    
    Usage:
        provider = ReplicateProvider()
        
        # Step 1: Create Cog package locally
        result = provider.export(
            "my_model",
            output_dir="./replicate_package"
        )
        
        # Step 2: Push to Replicate (requires Cog CLI)
        # cd ./replicate_package && cog push r8.im/username/model
    """
    
    NAME = "replicate"
    DESCRIPTION = "Export to Replicate - run models via API in the cloud"
    REQUIRES_AUTH = True
    AUTH_ENV_VAR = "REPLICATE_API_TOKEN"
    SUPPORTED_FORMATS = ["pytorch", "cog"]
    WEBSITE = "https://replicate.com"
    
    def _create_cog_yaml(
        self,
        output_path: Path,
        model_name: str,
        config: dict[str, Any]
    ):
        """Create cog.yaml configuration file."""
        cog_config = f"""# Cog configuration for {model_name}
# https://github.com/replicate/cog/blob/main/docs/yaml.md

build:
  python_version: "3.11"
  python_packages:
    - "torch>=2.0"
    - "safetensors"
  gpu: true

predict: "predict.py:Predictor"
"""
        with open(output_path / "cog.yaml", "w") as f:
            f.write(cog_config)
    
    def _create_predict_py(
        self,
        output_path: Path,
        model_name: str,
        config: dict[str, Any]
    ):
        """Create predict.py for Cog."""
        predict_code = f'''"""
Cog predictor for {model_name}
Generated by Enigma AI Engine
"""

import torch
from cog import BasePredictor, Input
from pathlib import Path


class Predictor(BasePredictor):
    def setup(self):
        """Load the model into memory."""
        import json
        
        # Load config
        with open("config.json") as f:
            self.config = json.load(f)
        
        # Build model from config
        from model import build_model
        self.model = build_model(self.config)
        
        # Load weights
        weights_path = Path("model.safetensors")
        if weights_path.exists():
            from safetensors.torch import load_file
            state_dict = load_file(str(weights_path))
        else:
            state_dict = torch.load("pytorch_model.bin", map_location="cuda", weights_only=True)
        
        self.model.load_state_dict(state_dict)
        self.model.to("cuda")
        self.model.eval()
    
    def predict(
        self,
        prompt: str = Input(description="Input text prompt"),
        max_tokens: int = Input(description="Maximum tokens to generate", default=100),
        temperature: float = Input(description="Sampling temperature", default=0.7, ge=0.0, le=2.0),
        top_p: float = Input(description="Top-p sampling", default=0.9, ge=0.0, le=1.0),
    ) -> str:
        """Generate text from prompt."""
        # Simple tokenization (replace with your tokenizer)
        # This is a placeholder - you should use your actual tokenizer
        
        with torch.no_grad():
            # Generate using your model's generate method
            # This is a simplified example
            output = self.model.generate(
                prompt,
                max_new_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p
            )
        
        return output
'''
        with open(output_path / "predict.py", "w") as f:
            f.write(predict_code)
    
    def _create_model_py(
        self,
        output_path: Path,
        config: dict[str, Any]
    ):
        """Create model.py with Forge architecture."""
        model_code = '''"""
Enigma AI Engine Model Architecture
Auto-generated for Replicate deployment
"""

import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Dict, Any


class RMSNorm(nn.Module):
    def __init__(self, dim: int, eps: float = 1e-6):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(dim))
        self.eps = eps
    
    def forward(self, x):
        norm = x.float().pow(2).mean(-1, keepdim=True).add(self.eps).rsqrt()
        return (x * norm).type_as(x) * self.weight


class RotaryEmbedding(nn.Module):
    def __init__(self, dim: int, max_seq_len: int = 2048, theta: float = 10000.0):
        super().__init__()
        inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2).float() / dim))
        self.register_buffer("inv_freq", inv_freq)
        self.max_seq_len = max_seq_len
    
    def forward(self, x, seq_len: int):
        t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
        freqs = torch.outer(t, self.inv_freq)
        emb = torch.cat((freqs, freqs), dim=-1)
        return emb.cos(), emb.sin()


def rotate_half(x):
    x1, x2 = x.chunk(2, dim=-1)
    return torch.cat((-x2, x1), dim=-1)


def apply_rotary_pos_emb(q, k, cos, sin):
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed


class Attention(nn.Module):
    def __init__(self, dim: int, n_heads: int, n_kv_heads: int):
        super().__init__()
        self.n_heads = n_heads
        self.n_kv_heads = n_kv_heads
        self.head_dim = dim // n_heads
        self.n_rep = n_heads // n_kv_heads
        
        self.wq = nn.Linear(dim, n_heads * self.head_dim, bias=False)
        self.wk = nn.Linear(dim, n_kv_heads * self.head_dim, bias=False)
        self.wv = nn.Linear(dim, n_kv_heads * self.head_dim, bias=False)
        self.wo = nn.Linear(n_heads * self.head_dim, dim, bias=False)
    
    def forward(self, x, cos, sin, mask=None):
        B, T, _ = x.shape
        
        q = self.wq(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)
        k = self.wk(x).view(B, T, self.n_kv_heads, self.head_dim).transpose(1, 2)
        v = self.wv(x).view(B, T, self.n_kv_heads, self.head_dim).transpose(1, 2)
        
        q, k = apply_rotary_pos_emb(q, k, cos.unsqueeze(0).unsqueeze(0), sin.unsqueeze(0).unsqueeze(0))
        
        if self.n_rep > 1:
            k = k.repeat_interleave(self.n_rep, dim=1)
            v = v.repeat_interleave(self.n_rep, dim=1)
        
        attn = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)
        if mask is not None:
            attn = attn + mask
        attn = F.softmax(attn, dim=-1)
        
        out = torch.matmul(attn, v)
        out = out.transpose(1, 2).contiguous().view(B, T, -1)
        return self.wo(out)


class FeedForward(nn.Module):
    def __init__(self, dim: int, hidden_dim: int, use_swiglu: bool = True):
        super().__init__()
        self.use_swiglu = use_swiglu
        if use_swiglu:
            self.w1 = nn.Linear(dim, hidden_dim, bias=False)
            self.w2 = nn.Linear(hidden_dim, dim, bias=False)
            self.w3 = nn.Linear(dim, hidden_dim, bias=False)
        else:
            self.w1 = nn.Linear(dim, hidden_dim, bias=False)
            self.w2 = nn.Linear(hidden_dim, dim, bias=False)
    
    def forward(self, x):
        if self.use_swiglu:
            return self.w2(F.silu(self.w1(x)) * self.w3(x))
        return self.w2(F.gelu(self.w1(x)))


class TransformerBlock(nn.Module):
    def __init__(self, dim: int, n_heads: int, n_kv_heads: int, hidden_dim: int):
        super().__init__()
        self.attention = Attention(dim, n_heads, n_kv_heads)
        self.feed_forward = FeedForward(dim, hidden_dim)
        self.attention_norm = RMSNorm(dim)
        self.ffn_norm = RMSNorm(dim)
    
    def forward(self, x, cos, sin, mask=None):
        x = x + self.attention(self.attention_norm(x), cos, sin, mask)
        x = x + self.feed_forward(self.ffn_norm(x))
        return x


class ForgeModel(nn.Module):
    def __init__(self, config: Dict[str, Any]):
        super().__init__()
        self.config = config
        
        vocab_size = config.get("vocab_size", 8000)
        dim = config.get("dim", 512)
        n_layers = config.get("n_layers", 8)
        n_heads = config.get("n_heads", 8)
        n_kv_heads = config.get("n_kv_heads", n_heads)
        hidden_dim = config.get("hidden_dim", dim * 4)
        max_seq_len = config.get("max_seq_len", 1024)
        
        self.embed = nn.Embedding(vocab_size, dim)
        self.rotary = RotaryEmbedding(dim // n_heads, max_seq_len)
        self.layers = nn.ModuleList([
            TransformerBlock(dim, n_heads, n_kv_heads, hidden_dim)
            for _ in range(n_layers)
        ])
        self.norm = RMSNorm(dim)
        self.output = nn.Linear(dim, vocab_size, bias=False)
    
    def forward(self, x, mask=None):
        B, T = x.shape
        h = self.embed(x)
        cos, sin = self.rotary(h, T)
        
        if mask is None:
            mask = torch.triu(torch.full((T, T), float("-inf"), device=x.device), diagonal=1)
        
        for layer in self.layers:
            h = layer(h, cos, sin, mask)
        
        h = self.norm(h)
        return self.output(h)
    
    def generate(self, prompt: str, max_new_tokens: int = 100, temperature: float = 0.7, top_p: float = 0.9):
        """Simple generation (replace with proper tokenizer)."""
        # This is a placeholder - implement proper tokenization
        return f"[Generated response for: {prompt}]"


def build_model(config: Dict[str, Any]) -> ForgeModel:
    """Build model from config."""
    # Handle HuggingFace-style config
    if "hidden_size" in config:
        config = {
            "vocab_size": config.get("vocab_size", 8000),
            "dim": config.get("hidden_size", 512),
            "n_layers": config.get("num_hidden_layers", 8),
            "n_heads": config.get("num_attention_heads", 8),
            "n_kv_heads": config.get("num_key_value_heads", 8),
            "hidden_dim": config.get("intermediate_size", 2048),
            "max_seq_len": config.get("max_position_embeddings", 1024),
        }
    # Use _forge_config if available
    if "_forge_config" in config:
        config = config["_forge_config"]
    
    return ForgeModel(config)
'''
        with open(output_path / "model.py", "w") as f:
            f.write(model_code)
    
    def export(
        self,
        model_name: str,
        output_dir: Optional[str] = None,
        model_owner: Optional[str] = None,
        push: bool = False,
        token: Optional[str] = None,
        **kwargs
    ) -> ExportResult:
        """
        Export to Replicate Cog format.
        
        Args:
            model_name: Enigma AI Engine model name
            output_dir: Directory for Cog package
            model_owner: Replicate username (for push)
            push: Whether to push to Replicate (requires Cog CLI)
            token: Replicate API token
        """
        try:
            model_path = self._get_model_path(model_name)
            
            # Default output directory
            if not output_dir:
                output_dir = str(self.models_dir / f"{model_name}_replicate")
            
            output_path = Path(output_dir)
            output_path.mkdir(parents=True, exist_ok=True)
            
            # Load model info
            config = self._load_config(model_path)
            metadata = self._load_metadata(model_path)
            
            # Create Cog package files
            self._create_cog_yaml(output_path, model_name, config)
            self._create_predict_py(output_path, model_name, config)
            self._create_model_py(output_path, config)
            
            # Copy config
            with open(output_path / "config.json", "w") as f:
                json.dump(config, f, indent=2)
            
            # Copy weights
            weights_src = model_path / "weights.pth"
            if weights_src.exists():
                # Try safetensors first
                try:
                    from safetensors.torch import save_file
                    state_dict = torch.load(weights_src, map_location="cpu", weights_only=True)
                    save_file(state_dict, output_path / "model.safetensors")
                except ImportError:
                    shutil.copy(weights_src, output_path / "pytorch_model.bin")
            
            # Create README
            readme = f"""# {model_name}

Enigma AI Engine model packaged for Replicate.

## Deploy to Replicate

1. Install Cog: https://github.com/replicate/cog#install
2. Build: `cog build`
3. Test: `cog predict -i prompt="Hello"`
4. Push: `cog push r8.im/{model_owner or 'your-username'}/{model_name}`

## Usage via API

```python
import replicate

output = replicate.run(
    "{model_owner or 'your-username'}/{model_name}",
    input={{"prompt": "Hello, how are you?"}}
)
print(output)
```
"""
            with open(output_path / "README.md", "w") as f:
                f.write(readme)
            
            # Optionally push to Replicate
            if push:
                if not HAVE_COG:
                    return ExportResult(
                        status=ExportStatus.PARTIAL,
                        provider=self.NAME,
                        model_name=model_name,
                        local_path=str(output_path),
                        message="Package created but Cog CLI not installed. Install from: https://github.com/replicate/cog"
                    )
                
                if not model_owner:
                    return ExportResult(
                        status=ExportStatus.PARTIAL,
                        provider=self.NAME,
                        model_name=model_name,
                        local_path=str(output_path),
                        message="Package created but model_owner required for push"
                    )
                
                # Push using Cog CLI
                import subprocess
                result = subprocess.run(
                    ["cog", "push", f"r8.im/{model_owner}/{model_name}"],
                    cwd=str(output_path),
                    capture_output=True,
                    text=True,
                    timeout=3600  # Model uploads can take a long time
                )
                
                if result.returncode != 0:
                    return ExportResult(
                        status=ExportStatus.PARTIAL,
                        provider=self.NAME,
                        model_name=model_name,
                        local_path=str(output_path),
                        message=f"Package created but push failed: {result.stderr}"
                    )
                
                url = f"https://replicate.com/{model_owner}/{model_name}"
                return ExportResult(
                    status=ExportStatus.SUCCESS,
                    provider=self.NAME,
                    model_name=model_name,
                    url=url,
                    local_path=str(output_path),
                    message=f"Pushed to {url}"
                )
            
            return ExportResult(
                status=ExportStatus.SUCCESS,
                provider=self.NAME,
                model_name=model_name,
                local_path=str(output_path),
                message=f"Cog package created at {output_path}. Run 'cog push' to deploy."
            )
            
        except Exception as e:
            logger.exception("Replicate export failed")
            return ExportResult(
                status=ExportStatus.FAILED,
                provider=self.NAME,
                model_name=model_name,
                message=str(e)
            )


class ReplicateImporter(ImportProvider):
    """
    Import/use models from Replicate.
    
    Replicate hosts thousands of models that you can run via API.
    This importer creates a local wrapper to use Replicate models.
    
    Usage:
        importer = ReplicateImporter()
        
        # Search for models
        results = importer.search("llama")
        
        # Import creates a local API wrapper
        result = importer.import_model("meta/llama-2-7b-chat")
    """
    
    NAME = "replicate"
    DESCRIPTION = "Import from Replicate - run cloud models via API"
    REQUIRES_AUTH = True
    AUTH_ENV_VAR = "REPLICATE_API_TOKEN"
    SUPPORTED_FORMATS = ["api"]
    WEBSITE = "https://replicate.com"
    
    def search(
        self,
        query: str,
        limit: int = 10,
        **kwargs
    ) -> list[dict[str, Any]]:
        """Search Replicate for models."""
        if not HAVE_REPLICATE:
            logger.warning("replicate not installed")
            return []
        
        try:
            # Replicate doesn't have a search API, but we can list collections
            # For now, return some popular models as suggestions
            popular_models = [
                {"id": "meta/llama-2-70b-chat", "name": "Llama 2 70B Chat", "description": "Meta's Llama 2 70B chat model"},
                {"id": "meta/llama-2-13b-chat", "name": "Llama 2 13B Chat", "description": "Meta's Llama 2 13B chat model"},
                {"id": "meta/llama-2-7b-chat", "name": "Llama 2 7B Chat", "description": "Meta's Llama 2 7B chat model"},
                {"id": "mistralai/mistral-7b-v0.1", "name": "Mistral 7B", "description": "Mistral AI's 7B model"},
                {"id": "stability-ai/sdxl", "name": "SDXL", "description": "Stability AI's SDXL image model"},
                {"id": "openai/whisper", "name": "Whisper", "description": "OpenAI's speech recognition"},
            ]
            
            # Filter by query
            query_lower = query.lower()
            filtered = [
                m for m in popular_models 
                if query_lower in m["id"].lower() or query_lower in m["name"].lower()
            ]
            
            return filtered[:limit] if filtered else popular_models[:limit]
            
        except Exception as e:
            logger.warning(f"Replicate search failed: {e}")
            return []
    
    def import_model(
        self,
        source_id: str,
        local_name: Optional[str] = None,
        token: Optional[str] = None,
        **kwargs
    ) -> ImportResult:
        """
        Create a local wrapper for a Replicate model.
        
        This doesn't download the model (Replicate models run in the cloud),
        but creates a local configuration to use it via API.
        
        Args:
            source_id: Replicate model ID (e.g., "meta/llama-2-7b-chat")
            local_name: Local name for the wrapper
            token: Replicate API token
        """
        if not HAVE_REPLICATE:
            return ImportResult(
                status=ImportStatus.FAILED,
                provider=self.NAME,
                model_name=local_name or source_id,
                source_id=source_id,
                message="replicate not installed. Run: pip install replicate"
            )
        
        try:
            token = self._check_auth(token)
            
            # Determine local name
            if not local_name:
                local_name = f"replicate_{source_id.replace('/', '_')}".lower()
            
            local_name = local_name.lower().strip().replace(" ", "_")
            model_path = self.models_dir / local_name
            
            if model_path.exists():
                return ImportResult(
                    status=ImportStatus.ALREADY_EXISTS,
                    provider=self.NAME,
                    model_name=local_name,
                    source_id=source_id,
                    local_path=str(model_path),
                    message=f"Model wrapper already exists at {model_path}"
                )
            
            model_path.mkdir(parents=True, exist_ok=True)
            
            # Create wrapper configuration
            config = {
                "source": "replicate",
                "model_id": source_id,
                "api_endpoint": f"https://api.replicate.com/v1/predictions",
                "type": "api_wrapper",
            }
            
            with open(model_path / "config.json", "w") as f:
                json.dump(config, f, indent=2)
            
            # Create a Python wrapper file
            wrapper_code = f'''"""
Replicate API Wrapper for {source_id}
Generated by Enigma AI Engine
"""

import os
try:
    import replicate
except ImportError:
    raise ImportError("pip install replicate")

MODEL_ID = "{source_id}"

def generate(prompt: str, **kwargs) -> str:
    """Generate text using Replicate API."""
    output = replicate.run(
        MODEL_ID,
        input={{"prompt": prompt, **kwargs}}
    )
    
    # Handle streaming output
    if hasattr(output, "__iter__") and not isinstance(output, str):
        return "".join(output)
    return output

def chat(message: str, system: str = "", **kwargs) -> str:
    """Chat interface."""
    if system:
        prompt = f"{{system}}\\n\\nUser: {{message}}\\nAssistant:"
    else:
        prompt = f"User: {{message}}\\nAssistant:"
    return generate(prompt, **kwargs)

if __name__ == "__main__":
    print(chat("Hello, how are you?"))
'''
            with open(model_path / "wrapper.py", "w") as f:
                f.write(wrapper_code)
            
            # Register
            self._register_model(
                local_name=local_name,
                model_path=model_path,
                source="replicate",
                source_id=source_id,
                metadata={"replicate_id": source_id, "type": "api_wrapper"}
            )
            
            return ImportResult(
                status=ImportStatus.SUCCESS,
                provider=self.NAME,
                model_name=local_name,
                source_id=source_id,
                local_path=str(model_path),
                message=f"Created Replicate wrapper for {source_id}. Use wrapper.py to interact."
            )
            
        except Exception as e:
            logger.exception("Replicate import failed")
            return ImportResult(
                status=ImportStatus.FAILED,
                provider=self.NAME,
                model_name=local_name or source_id,
                source_id=source_id,
                message=str(e)
            )