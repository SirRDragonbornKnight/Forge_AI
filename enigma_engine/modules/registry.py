"""
================================================================================
üì¶ MODULE REGISTRY - THE CATALOG OF ALL MODULES
================================================================================

Central registry where ALL available modules are defined! This is the "catalog"
that tells ModuleManager what modules exist and how to load them.

üìç FILE: enigma_engine/modules/registry.py
üè∑Ô∏è TYPE: Module Definitions & Registration
üéØ MAIN CLASSES: ModelModule, TokenizerModule, ImageGenLocalModule, etc.

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  AVAILABLE MODULES:                                                         ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  üß† CORE:                                                                    ‚îÇ
‚îÇ     ‚Ä¢ ModelModule       - Forge transformer (enigma_engine/core/model.py)       ‚îÇ
‚îÇ     ‚Ä¢ TokenizerModule   - Text tokenizer (enigma_engine/core/tokenizer.py)      ‚îÇ
‚îÇ     ‚Ä¢ InferenceModule   - Text generation (enigma_engine/core/inference.py)     ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  üé® GENERATION (LOCAL vs API - pick one!):                                  ‚îÇ
‚îÇ     ‚Ä¢ ImageGenLocalModule  / ImageGenAPIModule                              ‚îÇ
‚îÇ     ‚Ä¢ CodeGenLocalModule   / CodeGenAPIModule                               ‚îÇ
‚îÇ     ‚Ä¢ VideoGenLocalModule  / VideoGenAPIModule                              ‚îÇ
‚îÇ     ‚Ä¢ AudioGenLocalModule  / AudioGenAPIModule                              ‚îÇ
‚îÇ     ‚Ä¢ ThreeDGenLocalModule / ThreeDGenAPIModule                             ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  üíæ MEMORY:                                                                  ‚îÇ
‚îÇ     ‚Ä¢ MemoryModule      - Conversation storage                              ‚îÇ
‚îÇ     ‚Ä¢ EmbeddingModule   - Vector embeddings                                 ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  üåê NETWORK:                                                                 ‚îÇ
‚îÇ     ‚Ä¢ APIServerModule   - REST API                                          ‚îÇ
‚îÇ     ‚Ä¢ NetworkModule     - Multi-device                                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚ö†Ô∏è MODULE CONFLICTS:
    ‚úó image_gen_local + image_gen_api   (same capability)
    ‚úó code_gen_local + code_gen_api     (same capability)
    ‚úó video_gen_local + video_gen_api   (same capability)

üîó CONNECTED FILES:
    ‚Üí USES:      enigma_engine/modules/manager.py (Module base class)
    ‚Üí WRAPS:     enigma_engine/core/*.py (core modules)
    ‚Üí WRAPS:     enigma_engine/gui/tabs/*.py (generation tabs)
    ‚Üê USED BY:   enigma_engine/modules/manager.py (discovers modules)

üìñ SEE ALSO:
    ‚Ä¢ enigma_engine/modules/manager.py       - Loads/unloads modules
    ‚Ä¢ enigma_engine/gui/tabs/modules_tab.py  - GUI for toggling
    ‚Ä¢ docs/MODULE_GUIDE.md              - Module documentation
"""

from __future__ import annotations

import logging
import os
from functools import lru_cache
from typing import TYPE_CHECKING, Any

from .manager import Module, ModuleCategory, ModuleInfo, ModuleManager

if TYPE_CHECKING:
    pass  # Type-only imports if needed

logger = logging.getLogger(__name__)


# =============================================================================
# üß† CORE MODULE DEFINITIONS
# =============================================================================
# These are the essential modules that power the AI:
# - ModelModule: The neural network brain
# - TokenizerModule: Text ‚Üí numbers converter
# - TrainingModule: Learn from data
# - InferenceModule: Generate responses

class ModelModule(Module):
    """
    Core transformer model module.
    
    üìñ WHAT THIS IS:
    The "brain" of Enigma AI Engine - a transformer neural network that understands
    and generates text. This wraps the Forge model from core/model.py.
    
    üìê CONFIGURATION OPTIONS:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ size        ‚îÇ nano, micro, tiny, small, medium, large, xl, titan  ‚îÇ
    ‚îÇ vocab_size  ‚îÇ Number of tokens (1000-500000, default 8000)        ‚îÇ
    ‚îÇ device      ‚îÇ auto, cuda, cpu, mps                                ‚îÇ
    ‚îÇ dtype       ‚îÇ float32, float16, bfloat16                          ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    
    üîó WRAPS: enigma_engine/core/model.py ‚Üí Forge class
    """

    INFO = ModuleInfo(
        id="model",
        name="Forge Model",
        description="Core transformer language model with RoPE, RMSNorm, SwiGLU, GQA",
        category=ModuleCategory.CORE,
        version="2.0.0",
        requires=[],  # No dependencies - this is the foundation!
        provides=[
            "language_model",      # Provides language understanding
            "model_embeddings"],   # Provides word vectors
        config_schema={
            "size": {
                "type": "choice",
                "options": [
                    "nano",     # ~1M params - Raspberry Pi
                    "micro",    # ~2M params - Light devices
                    "tiny",     # ~5M params - Entry level
                    "small",    # ~27M params - Desktop default
                    "medium",   # ~85M params - Good balance
                    "large",    # ~300M params - Quality focus
                    "xl",       # ~1B params - Powerful
                    "xxl",      # ~3B params - Very powerful
                    "titan"],   # ~7B+ params - Datacenter
                "default": "small"},
            "vocab_size": {
                "type": "int",
                        "min": 1000,
                        "max": 500000,
                        "default": 8000},
            "device": {
                "type": "choice",
                "options": [
                    "auto",   # Auto-detect best
                    "cuda",   # NVIDIA GPU
                    "cpu",    # CPU (slow but works everywhere)
                    "mps"],   # Apple Silicon GPU
                "default": "auto"},
            "dtype": {
                "type": "choice",
                "options": [
                    "float32",   # Full precision (most compatible)
                    "float16",   # Half precision (faster, less VRAM)
                    "bfloat16"], # Brain float (best for training)
                "default": "float32"},
        },
        min_ram_mb=512,  # Minimum 512MB RAM required
    )

    def load(self) -> bool:
        """
        Load the Forge model into memory.
        
        üìñ WHAT HAPPENS:
        1. Import the model factory from core/model.py
        2. Create a model of the specified size
        3. Store it in self._instance for later use
        """
        from enigma_engine.core.model import create_model

        # Try to use progress tracking
        try:
            from enigma_engine.utils.progress import ProgressTracker, model_loading_stages
            progress = ProgressTracker("Loading model", total=100)
            stages = model_loading_stages()
            progress.start()
            progress.update(current=stages["init"][0], status=stages["init"][1])
        except ImportError:
            progress = None
            stages = None

        size = self.config.get('size', 'small')
        vocab_size = self.config.get('vocab_size', 8000)
        
        if progress:
            progress.update(current=stages["config"][0], status=stages["config"][1])

        # Create the model - this allocates the neural network weights
        if progress:
            progress.update(current=stages["weights"][0], status=f"Creating {size} model")
        
        self._instance = create_model(size, vocab_size=vocab_size)
        
        if progress:
            if self._instance is not None:
                progress.update(current=stages["ready"][0], status=stages["ready"][1])
                progress.finish("Model loaded successfully")
            else:
                progress.finish("Model loading failed")
        
        return self._instance is not None

    def unload(self) -> bool:
        """
        Unload the model and free memory.
        
        üìñ WHY THIS MATTERS:
        Models can use gigabytes of memory. When unloading,
        we delete the reference so Python can garbage collect it.
        We also explicitly clear GPU cache if available.
        """
        if self._instance is not None:
            del self._instance
            self._instance = None
            # Force GPU memory cleanup
            try:
                import torch
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            except Exception as e:
                logger.debug(f"GPU cache cleanup skipped: {e}")
        return True


class TokenizerModule(Module):
    """
    Tokenizer module - converts text to/from tokens.
    
    üìñ WHAT THIS DOES:
    AI models don't understand text directly - they work with numbers.
    The tokenizer converts:
      "Hello world" ‚Üí [15496, 995]  (encode)
      [15496, 995] ‚Üí "Hello world"  (decode)
    
    üìê TOKENIZER TYPES:
    - auto: Best available (tries BPE first)
    - bpe: Byte-Pair Encoding (like GPT)
    - character: One token per character
    - simple: Whitespace-based splitting
    
    üîó WRAPS: enigma_engine/core/tokenizer.py
    """

    INFO = ModuleInfo(
        id="tokenizer",
        name="Tokenizer",
        description="Text tokenization with BPE, character, or custom vocabularies",
        category=ModuleCategory.CORE,
        version="2.0.0",
        requires=[],
        provides=[
            "tokenization",  # Can convert text ‚Üî tokens
            "vocabulary"],   # Has a word list
        config_schema={
            "type": {
                "type": "choice",
                "options": [
                    "auto",       # Auto-detect best
                    "bpe",        # Byte-Pair Encoding
                    "character",  # Character-level
                    "simple"],    # Whitespace split
                "default": "auto"},
            "vocab_size": {
                "type": "int",
                        "min": 100,
                        "max": 500000,
                        "default": 8000},
        },
    )

    def load(self) -> bool:
        """Load the tokenizer."""
        from enigma_engine.core.tokenizer import get_tokenizer

        tok_type = self.config.get('type', 'auto')
        self._instance = get_tokenizer(tok_type)
        return self._instance is not None


class TrainingModule(Module):
    """
    Training module - trains models on data.
    
    üìñ WHAT THIS DOES:
    Takes text data and teaches the model to predict the next word.
    Over many iterations, the model learns patterns in language.
    
    üìê TRAINING FEATURES:
    - AMP: Automatic Mixed Precision (faster training)
    - Gradient Accumulation: Train larger batches on limited VRAM
    - Distributed: Train across multiple GPUs
    
    üìê KEY PARAMETERS:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ learning_rate  ‚îÇ How fast to learn (1e-6 to 0.1, default 3e-4)    ‚îÇ
    ‚îÇ batch_size     ‚îÇ Examples per step (1-256, default 8)              ‚îÇ
    ‚îÇ epochs         ‚îÇ Times through data (1-10000, default 30)          ‚îÇ
    ‚îÇ use_amp        ‚îÇ Mixed precision? (default True)                   ‚îÇ
    ‚îÇ gradient_accumulation ‚îÇ Steps before update (1-64, default 4)     ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    
    üîó WRAPS: enigma_engine/core/training.py ‚Üí Trainer class
    """

    INFO = ModuleInfo(
        id="training",
        name="Training System",
        description="Production-grade training with AMP, gradient accumulation, distributed support",
        category=ModuleCategory.CORE,
        version="2.0.0",
        requires=[
            "model",      # Need a model to train!
            "tokenizer"], # Need tokenizer to process text
        provides=[
            "model_training",   # Can train models
            "fine_tuning"],     # Can fine-tune existing models
        supports_distributed=True,  # Can use multiple GPUs
        config_schema={
            "learning_rate": {
                "type": "float",
                "min": 1e-6,
                "max": 1e-1,
                "default": 3e-4},
            "batch_size": {
                "type": "int",
                "min": 1,
                        "max": 256,
                        "default": 8},
            "epochs": {
                "type": "int",
                "min": 1,
                "max": 10000,
                "default": 30},
            "use_amp": {
                "type": "bool",
                "default": True},
            "gradient_accumulation": {
                "type": "int",
                "min": 1,
                "max": 64,
                "default": 4},
        },
    )

    def load(self) -> bool:
        from enigma_engine.core.training import Trainer, TrainingConfig
        self._trainer_class = Trainer
        self._config_class = TrainingConfig
        return True

    def get_interface(self):
        return {
            'Trainer': self._trainer_class,
            'TrainingConfig': self._config_class,
        }


class InferenceModule(Module):
    """
    Inference module - generates text from models.
    
    üìñ WHAT THIS DOES:
    Takes a trained model and generates text from it.
    This is the "thinking" part - you give it a prompt, it gives you a response.
    
    üìê KEY PARAMETERS:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ max_length   ‚îÇ Maximum tokens to generate (1-32768, default 2048) ‚îÇ
    ‚îÇ temperature  ‚îÇ Randomness (0=deterministic, 2=chaos, default 0.8) ‚îÇ
    ‚îÇ top_k        ‚îÇ Consider only top K tokens (0-1000, default 50)    ‚îÇ
    ‚îÇ top_p        ‚îÇ Nucleus sampling (0-1, default 0.9)                ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    
    üìê SAMPLING STRATEGY GUIDE:
    - Creative writing: temperature=1.0, top_p=0.95
    - Code generation: temperature=0.3, top_k=40
    - Factual answers: temperature=0.1, top_p=0.8
    
    üîó WRAPS: enigma_engine/core/inference.py ‚Üí EnigmaEngine class
    """

    INFO = ModuleInfo(
        id="inference",
        name="Inference Engine",
        description="High-performance text generation with streaming, batching, chat",
        category=ModuleCategory.CORE,
        version="2.0.0",
        requires=["model", "tokenizer"],  # Needs model to run and tokenizer to decode
        provides=[
            "text_generation",  # Can generate text
            "streaming",        # Can stream output token by token
            "chat"],            # Can have conversations
        config_schema={
            "max_length": {"type": "int", "min": 1, "max": 32768, "default": 2048},
            "temperature": {"type": "float", "min": 0.0, "max": 2.0, "default": 0.8},
            "top_k": {"type": "int", "min": 0, "max": 1000, "default": 50},
            "top_p": {"type": "float", "min": 0.0, "max": 1.0, "default": 0.9},
        },
    )

    def load(self) -> bool:
        """Load the EnigmaEngine class for local inference."""
        from enigma_engine.core.inference import EnigmaEngine
        self._engine_class = EnigmaEngine
        return True


# =============================================================================
# ‚òÅÔ∏è CLOUD/API MODULES
# =============================================================================
# These modules use external APIs instead of local models.
# Great for:
# - Raspberry Pi (not enough power for local models)
# - Access to powerful models (GPT-4, Claude)
# - No training required

class ChatAPIModule(Module):
    """
    Cloud/API chat via OpenAI, Anthropic, Ollama, or other providers.
    
    üìñ WHAT THIS DOES:
    Instead of running a model locally, this sends your prompts to
    cloud APIs and gets responses back. Perfect for:
    - Low-power devices (Raspberry Pi)
    - Access to powerful models (GPT-4, Claude)
    - No training or GPU needed
    
    üìê PROVIDER OPTIONS:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ ollama    ‚îÇ FREE! Run Llama/Mistral locally, no API key needed    ‚îÇ
    ‚îÇ openai    ‚îÇ GPT-4, GPT-3.5 (paid, needs API key)                  ‚îÇ
    ‚îÇ anthropic ‚îÇ Claude models (paid, needs API key)                   ‚îÇ
    ‚îÇ google    ‚îÇ Gemini (free tier available)                          ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    
    üìê OLLAMA MODELS (FREE):
    - llama3.2:1b - Fastest, works on 4GB RAM
    - llama3.2:3b - Good balance
    - mistral:7b - Quality responses
    - phi3:mini - Microsoft's small model
    - gemma2:2b - Google's efficient model
    
    ‚ö†Ô∏è CONFLICTS: Can't use with local inference module
    """

    INFO = ModuleInfo(
        id="chat_api",
        name="Cloud/API Chat",
        description="Chat via API (Ollama FREE, GPT-4, Claude). No training needed!",
        category=ModuleCategory.CORE,
        version="1.0.0",
        requires=[],
        conflicts=["inference"],  # Can't use both local and cloud inference
        provides=["text_generation", "streaming", "chat", "cloud_chat"],
        is_cloud_service=True,
        config_schema={
            "provider": {
                "type": "choice",
                "options": ["ollama", "openai", "anthropic", "google"],
                "default": "ollama"
            },
            "model": {
                "type": "choice",
                "options": [
                    # Ollama (FREE - local)
                    "llama3.2:1b",
                    "llama3.2:3b",
                    "mistral:7b",
                    "phi3:mini",
                    "gemma2:2b",
                    # OpenAI (paid)
                    "gpt-4o",
                    "gpt-4-turbo",
                    "gpt-3.5-turbo",
                    # Anthropic (paid)
                    "claude-sonnet-4-20250514",
                    "claude-opus-4-20250514",
                    "claude-3-opus-20240229",
                    "claude-3-sonnet-20240229",
                    "claude-3-haiku-20240307",
                    # Google (free tier available)
                    "gemini-pro"
                ],
                "default": "llama3.2:1b"
            },
            "api_key": {"type": "secret", "default": ""},
            "ollama_url": {"type": "string", "default": "http://localhost:11434"},
            "max_tokens": {"type": "int", "min": 100, "max": 128000, "default": 4096},
            "temperature": {"type": "float", "min": 0.0, "max": 2.0, "default": 0.7},
        },
    )

    def __init__(self, manager: ModuleManager = None, config: Dict[str, Any] = None):
        super().__init__(manager, config)
        self._client = None
        self._provider = None

    def load(self) -> bool:
        provider = self.config.get('provider', 'ollama')
        
        try:
            if provider == 'ollama':
                # Ollama is FREE and runs locally - no API key needed!
                import requests
                ollama_url = self.config.get('ollama_url', 'http://localhost:11434')
                # Test connection
                try:
                    resp = requests.get(f"{ollama_url}/api/tags", timeout=5)
                    if resp.status_code == 200:
                        self._client = ollama_url
                        self._provider = 'ollama'
                        logger.info(f"Connected to Ollama at {ollama_url}")
                        return True
                    else:
                        logger.warning(f"Ollama not responding. Install from: https://ollama.ai")
                        return False
                except requests.exceptions.ConnectionError:
                    logger.warning(
                        f"Could not connect to Ollama at {ollama_url}. "
                        f"Install Ollama from https://ollama.ai and run: ollama run llama3.2:1b"
                    )
                    return False
                    
            elif provider == 'openai':
                api_key = self.config.get('api_key') or os.environ.get('OPENAI_API_KEY')
                if not api_key:
                    logger.warning("No OpenAI API key. Set in Settings > API Keys")
                    return False
                import openai
                self._client = openai.OpenAI(api_key=api_key)
                
            elif provider == 'anthropic':
                api_key = self.config.get('api_key') or os.environ.get('ANTHROPIC_API_KEY')
                if not api_key:
                    logger.warning("No Anthropic API key. Set in Settings > API Keys")
                    return False
                import anthropic
                self._client = anthropic.Anthropic(api_key=api_key)
                
            elif provider == 'google':
                api_key = self.config.get('api_key') or os.environ.get('GOOGLE_API_KEY')
                if not api_key:
                    logger.warning("No Google API key. Get free key at: https://makersuite.google.com")
                    return False
                import google.generativeai as genai
                genai.configure(api_key=api_key)
                self._client = genai.GenerativeModel(self.config.get('model', 'gemini-pro'))
            
            self._provider = provider
            logger.info(f"Loaded {provider} chat")
            return True
        except ImportError as e:
            logger.warning(f"Missing library for {provider}: {e}")
            return False
        except Exception as e:
            logger.warning(f"Could not load {provider} client: {e}")
        
        # Fall back to built-in chat
        try:
            from enigma_engine.builtin import BuiltinChat
            self._client = BuiltinChat()
            if self._client.load():
                self._provider = 'builtin'
                logger.info("Using built-in chat (pattern-based)")
                return True
        except Exception as e2:
            logger.warning(f"Built-in chat also failed: {e2}")
        
        return False

    def chat(self, message: str, history: list = None) -> str:
        """Send a chat message and get a response."""
        if not self._client:
            return "Chat API not initialized. Check connection/API key."
        
        model = self.config.get('model', 'llama3.2:1b')
        max_tokens = self.config.get('max_tokens', 4096)
        temperature = self.config.get('temperature', 0.7)
        
        try:
            if self._provider == 'ollama':
                import requests
                response = requests.post(
                    f"{self._client}/api/chat",
                    json={
                        "model": model,
                        "messages": [{"role": "user", "content": message}],
                        "stream": False,
                        "options": {"temperature": temperature}
                    },
                    timeout=120
                )
                if response.status_code == 200:
                    return response.json().get("message", {}).get("content", "No response")
                else:
                    return f"Ollama error: {response.text}"
                    
            elif self._provider == 'openai':
                messages = [{"role": "system", "content": "You are a helpful AI assistant."}]
                if history:
                    messages.extend(history)
                messages.append({"role": "user", "content": message})
                
                response = self._client.chat.completions.create(
                    model=model,
                    messages=messages,
                    max_tokens=max_tokens,
                    temperature=temperature
                )
                return response.choices[0].message.content
                
            elif self._provider == 'anthropic':
                messages = []
                if history:
                    messages.extend(history)
                messages.append({"role": "user", "content": message})
                
                response = self._client.messages.create(
                    model=model,
                    max_tokens=max_tokens,
                    messages=messages
                )
                return response.content[0].text
                
            elif self._provider == 'google':
                response = self._client.generate_content(message)
                return response.text
                
        except Exception as e:
            logger.error(f"Cloud chat error: {e}")
            return f"Error: {e}"

    def generate(self, prompt: str, **kwargs) -> str:
        """Generate text (alias for chat without history)."""
        return self.chat(prompt)

    def unload(self) -> bool:
        self._client = None
        self._provider = None
        return True


class GGUFLoaderModule(Module):
    """
    GGUF model loader module - load llama.cpp compatible models.
    
    üìñ WHAT THIS IS:
    GGUF is a format for quantized models that run efficiently on CPUs.
    This lets you run Llama, Mistral, etc. without a GPU!
    
    üìê CONFIGURATION:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ model_path    ‚îÇ Path to .gguf file                                ‚îÇ
    ‚îÇ n_ctx         ‚îÇ Context window (512-32768, default 2048)          ‚îÇ
    ‚îÇ n_gpu_layers  ‚îÇ GPU layers (0=CPU only, higher=more GPU)          ‚îÇ
    ‚îÇ n_threads     ‚îÇ CPU threads (1-32, default 4)                     ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    
    ‚ö†Ô∏è CONFLICTS: Can't use with model or inference modules
    üîó WRAPS: enigma_engine/core/gguf_loader.py ‚Üí GGUFModel class
    """

    INFO = ModuleInfo(
        id="gguf_loader",
        name="GGUF Model Loader",
        description="Load and run GGUF format models (llama.cpp compatible)",
        category=ModuleCategory.CORE,
        version="1.0.0",
        requires=[],
        conflicts=["model", "inference"],  # Can't use both GGUF and Forge model
        provides=["text_generation", "completion", "gguf_support"],
        config_schema={
            "model_path": {"type": "string", "default": ""},
            "n_ctx": {"type": "int", "min": 512, "max": 32768, "default": 2048},
            "n_gpu_layers": {"type": "int", "min": 0, "max": 999, "default": 0},
            "n_threads": {"type": "int", "min": 1, "max": 32, "default": 4},
        },
    )

    def load(self) -> bool:
        """
        Load a GGUF model into memory.
        
        üìñ HOW IT WORKS:
        1. Get the path to the .gguf file from config
        2. Create a GGUFModel wrapper
        3. Load the model (may take time for large models)
        """
        try:
            from enigma_engine.core.gguf_loader import GGUFModel
            model_path = self.config.get('model_path', '')
            if not model_path:
                logger.info("No GGUF model path specified - ready for deferred loading")
                return True  # Ready to load when user provides path
            
            # Create and load the GGUF model wrapper
            self._instance = GGUFModel(
                model_path=model_path,
                n_ctx=self.config.get('n_ctx', 2048),
                n_gpu_layers=self.config.get('n_gpu_layers', 0),
                n_threads=self.config.get('n_threads', 4)
            )
            return self._instance.load()
        except Exception as e:
            logger.warning(f"Could not load GGUF model: {e}")
            return False

    def unload(self) -> bool:
        """Unload the GGUF model and free memory."""
        if self._instance:
            self._instance.unload()
            self._instance = None
            # Force GPU memory cleanup
            try:
                import torch
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            except Exception as e:
                logger.debug(f"GPU cache cleanup skipped: {e}")
        return True


# =============================================================================
# üíæ MEMORY MODULES
# =============================================================================
# These modules handle storing and retrieving information.

class MemoryModule(Module):
    """
    Memory module - conversation and knowledge storage.
    
    üìñ WHAT THIS DOES:
    Stores chat history so the AI remembers previous conversations.
    Can use different backends:
    - JSON: Simple file storage (good for small datasets)
    - SQLite: Database (good for large histories)
    - Vector: Semantic search (find similar conversations)
    
    üìê CONFIGURATION:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ backend           ‚îÇ json, sqlite, or vector                       ‚îÇ
    ‚îÇ max_conversations ‚îÇ How many to keep (default 1000)               ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    
    üîó WRAPS: enigma_engine/memory/manager.py ‚Üí ConversationManager class
    """

    INFO = ModuleInfo(
        id="memory",
        name="Memory System",
        description="Conversation history, vector search, long-term memory",
        category=ModuleCategory.MEMORY,
        version="1.0.0",
        requires=[],
        optional=["model"],  # For generating embeddings
        provides=[
            "conversation_storage",  # Can save/load conversations
            "vector_search",         # Can search by similarity
            "knowledge_base"],       # Can store facts
        config_schema={
            "backend": {"type": "choice", "options": ["json", "sqlite", "vector"], "default": "json"},
            "max_conversations": {"type": "int", "min": 1, "max": 100000, "default": 1000},
        },
    )

    def load(self) -> bool:
        """Load the conversation manager."""
        from enigma_engine.memory.manager import ConversationManager
        self._instance = ConversationManager()
        return True


# =============================================================================
# üé§ PERCEPTION MODULES (Input)
# =============================================================================
# These modules handle input from the real world.

class VoiceInputModule(Module):
    """
    Voice input module - speech to text.
    
    üìñ WHAT THIS DOES:
    Listens to your microphone and converts speech to text.
    This lets you talk to the AI instead of typing!
    
    üìê ENGINE OPTIONS:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ system  ‚îÇ Uses your OS's built-in speech recognition              ‚îÇ
    ‚îÇ whisper ‚îÇ OpenAI's Whisper (very accurate, needs GPU)             ‚îÇ
    ‚îÇ vosk    ‚îÇ Offline recognition (works without internet)            ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    
    üîó WRAPS: enigma_engine/voice/stt_simple.py
    """

    INFO = ModuleInfo(
        id="voice_input",
        name="Voice Input (STT)",
        description="Speech-to-text for voice commands and dictation",
        category=ModuleCategory.PERCEPTION,
        version="1.0.0",
        requires=[],
        provides=[
            "speech_recognition",  # Can convert speech to text
            "voice_commands"],     # Can understand voice commands
        config_schema={
            "engine": {
                "type": "choice",
                "options": [
                    "system",   # OS built-in
                    "whisper",  # OpenAI Whisper
                    "vosk"],    # Offline
                "default": "system"},
            "language": {
                "type": "string",
                        "default": "en-US"},
        },
    )

    def load(self) -> bool:
        """
        Load the speech-to-text system.
        
        üìñ HOW IT WORKS:
        Wraps the STT functions in a simple class that has a listen() method.
        """
        try:
            from enigma_engine.voice.stt_simple import (
                transcribe_from_file,
                transcribe_from_mic,
            )

            # Wrap functions in a simple class for consistency
            class STTWrapper:
                def __init__(self):
                    self.transcribe_from_mic = transcribe_from_mic
                    self.transcribe_from_file = transcribe_from_file
                
                def listen(self, timeout=8):
                    """Listen for speech and return text."""
                    return transcribe_from_mic(timeout)
            
            self._instance = STTWrapper()
            return True
        except Exception as e:
            logger.warning(f"Could not load voice input: {e}")
            return False


# =============================================================================
# üîä OUTPUT MODULES
# =============================================================================

class VoiceOutputModule(Module):
    """
    Voice output module - text to speech.
    
    üìñ WHAT THIS DOES:
    Converts text to spoken audio so the AI can talk back to you!
    
    üìê ENGINE OPTIONS:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ pyttsx3    ‚îÇ Offline TTS (works without internet, robotic voice)  ‚îÇ
    ‚îÇ elevenlabs ‚îÇ Cloud TTS (very natural, paid API)                   ‚îÇ
    ‚îÇ coqui      ‚îÇ Local neural TTS (good quality, needs GPU)           ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    """

    INFO = ModuleInfo(
        id="voice_output",
        name="Voice Output (TTS)",
        description="Text-to-speech for spoken responses",
        category=ModuleCategory.OUTPUT,
        version="1.0.0",
        requires=[],
        provides=[
            "text_to_speech",
            "spoken_response"],
        config_schema={
            "engine": {
                "type": "choice",
                "options": [
                    "system",
                    "pyttsx3",
                    "elevenlabs"],
                "default": "system"},
            "voice": {
                "type": "string",
                        "default": "default"},
            "rate": {
                "type": "float",
                "min": 0.5,
                "max": 2.0,
                "default": 1.0},
        },
    )

    def load(self) -> bool:
        try:
            from enigma_engine.voice.tts_simple import speak

            # Wrap function in a simple class for consistency
            class TTSWrapper:
                def __init__(self):
                    self._speak = speak
                
                def speak(self, text):
                    return self._speak(text)
                
                def say(self, text):
                    return self._speak(text)
            
            self._instance = TTSWrapper()
            return True
        except Exception as e:
            logger.warning(f"Could not load voice output: {e}")
            return False


class VisionModule(Module):
    """
    Vision module - image processing and analysis.
    
    üìñ WHAT THIS DOES:
    Gives the AI "eyes" to see and understand images. Can:
    - Capture from webcam/screen
    - Read text in images (OCR)
    - Detect objects in images
    
    üìê CONFIGURATION:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ camera_id  ‚îÇ Which camera to use (0 = default webcam)             ‚îÇ
    ‚îÇ resolution ‚îÇ 480p, 720p, or 1080p                                 ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    
    üîó WRAPS: enigma_engine/tools/vision.py ‚Üí ScreenVision class
    """

    INFO = ModuleInfo(
        id="vision",
        name="Vision System",
        description="Image capture, OCR, object detection, visual understanding",
        category=ModuleCategory.PERCEPTION,
        version="1.0.0",
        requires=[],
        optional=["model"],  # Model can help with image understanding
        provides=[
            "image_capture",     # Can grab images
            "ocr",               # Can read text in images
            "object_detection"], # Can find objects
        config_schema={
            "camera_id": {
                "type": "int",
                "min": 0,
                "max": 10,
                "default": 0},
            "resolution": {
                "type": "choice",
                "options": [
                        "480p",    # 640x480 - Fast
                        "720p",    # 1280x720 - Balanced
                        "1080p"],  # 1920x1080 - Quality
                "default": "720p"},
        },
    )

    def load(self) -> bool:
        """Load the vision system."""
        try:
            from enigma_engine.tools.vision import ScreenVision
            self._instance = ScreenVision()
            return True
        except Exception as e:
            logger.warning(f"Could not load vision: {e}")
            return False


class AvatarModule(Module):
    """
    Avatar module - visual AI representation.
    
    üìñ WHAT THIS DOES:
    Gives the AI a face! Can display emotions, lip-sync with speech,
    and show animations. Makes the AI feel more human.
    
    üìê STYLES:
    - 2d: Simple 2D character image
    - 3d: 3D rendered character
    - animated: Full animation with expressions
    
    üîó WRAPS: enigma_engine/avatar/controller.py ‚Üí AvatarController class
    """

    INFO = ModuleInfo(
        id="avatar",
        name="Avatar System",
        description="Visual representation, expressions, animations",
        category=ModuleCategory.OUTPUT,
        version="1.0.0",
        requires=[],
        optional=["voice_output"],  # Can lip-sync with voice
        provides=[
            "visual_avatar",  # Can show a character
            "expressions",    # Can show emotions
            "lip_sync"],      # Can move lips with speech
        config_schema={
            "style": {"type": "choice", "options": ["2d", "3d", "animated"], "default": "2d"},
            "character": {"type": "string", "default": "default"},
        },
    )

    def load(self) -> bool:
        """Load the avatar controller."""
        try:
            from enigma_engine.avatar.controller import AvatarController
            self._instance = AvatarController()
            return True
        except Exception as e:
            logger.warning(f"Could not load avatar: {e}")
            return False


# =============================================================================
# üîß TOOL MODULES
# =============================================================================
# These modules let the AI interact with the outside world.

class WebToolsModule(Module):
    """
    Web tools module - internet access.
    
    üìñ WHAT THIS DOES:
    Lets the AI search the web, fetch web pages, and call APIs.
    This is how the AI can get up-to-date information!
    
    ‚ö†Ô∏è SECURITY: Has safety limits on URLs and timeout
    
    üîó WRAPS: enigma_engine/tools/web_tools.py ‚Üí WebTools class
    """

    INFO = ModuleInfo(
        id="web_tools",
        name="Web Tools",
        description="Web search, page fetching, API access",
        category=ModuleCategory.TOOLS,
        version="1.0.0",
        requires=[],
        provides=[
            "web_search",   # Can search the web
            "url_fetch",    # Can download web pages
            "api_access"],  # Can call REST APIs
        config_schema={
            "allow_external": {"type": "bool", "default": True},  # Allow internet access?
            "timeout": {"type": "int", "min": 1, "max": 300, "default": 30},  # Timeout in seconds
        },
    )

    def load(self) -> bool:
        """Load web tools."""
        try:
            from enigma_engine.tools.web_tools import WebTools
            self._instance = WebTools()
            return True
        except ImportError as e:
            logger.debug(f"WebTools not available (optional): {e}")
            return True  # Optional module - don't fail if unavailable
        except Exception as e:
            logger.warning(f"WebTools failed to initialize: {e}")
            return False  # Unexpected error - report failure


class FileToolsModule(Module):
    """
    File tools module - file system access.
    
    üìñ WHAT THIS DOES:
    Lets the AI read and write files on your computer.
    
    ‚ö†Ô∏è SECURITY: Has a blocked paths list to prevent access to sensitive files
    (see enigma_engine/utils/security.py)
    
    üîó WRAPS: enigma_engine/tools/file_tools.py
    """

    INFO = ModuleInfo(
        id="file_tools",
        name="File Tools",
        description="Read, write, search files and directories",
        category=ModuleCategory.TOOLS,
        version="1.0.0",
        requires=[],
        provides=[
            "file_read",    # Can read file contents
            "file_write",   # Can write to files
            "file_search"], # Can search for files
        config_schema={
            "allowed_paths": {"type": "list", "default": []},
            "max_file_size_mb": {"type": "int", "min": 1, "max": 1000, "default": 100},
        },
    )

    def load(self) -> bool:
        try:
            from enigma_engine.tools.file_tools import FileTools
            self._instance = FileTools()
            return True
        except ImportError as e:
            logger.debug(f"FileTools not available (optional): {e}")
            return True  # Optional module
        except Exception as e:
            logger.warning(f"FileTools failed to initialize: {e}")
            return False


class ToolRouterModule(Module):
    """
    Tool router module - specialized model routing.
    
    üìñ WHAT THIS DOES:
    Routes different types of requests to specialized models:
    - Intent classification: "What does the user want?"
    - Vision: "What's in this image?"
    - Code: "Write me Python code"
    
    üìê HOW ROUTING WORKS:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ User Request ‚Üí Router Model ‚Üí "code" intent ‚Üí Code Model          ‚îÇ
    ‚îÇ                            ‚Üí "chat" intent ‚Üí Chat Model           ‚îÇ
    ‚îÇ                            ‚Üí "vision" intent ‚Üí Vision Model       ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    
    üîó WRAPS: enigma_engine/core/tool_router.py ‚Üí ToolRouter class
    """

    INFO = ModuleInfo(
        id="tool_router",
        name="Tool Router",
        description="Route requests to specialized models (intent classification, vision, code)",
        category=ModuleCategory.TOOLS,
        version="1.0.0",
        requires=["tokenizer"],
        optional=["model"],  # Uses model if available
        provides=[
            "intent_classification",   # Can classify what user wants
            "specialized_routing",     # Can route to right model
            "model_routing"],          # Can manage multiple models
        config_schema={
            "use_specialized": {
                "type": "bool",
                "default": True,
                "description": "Use specialized models for routing"
            },
            "enable_router": {
                "type": "bool",
                "default": True,
                "description": "Enable intent router model"
            },
            "enable_vision": {
                "type": "bool",
                "default": True,
                "description": "Enable vision captioning model"
            },
            "enable_code": {
                "type": "bool",
                "default": True,
                "description": "Enable code generation model"
            },
        },
    )

    def load(self) -> bool:
        """Load the tool router with specialized model support."""
        try:
            from enigma_engine.core.tool_router import get_router
            use_specialized = self.config.get('use_specialized', True)
            self._instance = get_router(use_specialized=use_specialized)
            logger.info(f"Tool router loaded (specialized: {use_specialized})")
            return True
        except Exception as e:
            logger.warning(f"Could not load tool router: {e}")
            return False

    def unload(self) -> bool:
        """Unload and clear cached models."""
        if self._instance is not None:
            # Clear any cached models to free memory
            if hasattr(self._instance, '_specialized_models'):
                self._instance._specialized_models.clear()
            if hasattr(self._instance, '_model_cache'):
                self._instance._model_cache.clear()
            self._instance = None
            # Force GPU memory cleanup
            try:
                import torch
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            except Exception as e:
                logger.debug(f"GPU cache cleanup skipped: {e}")
        return True


# =============================================================================
# üåê INTERFACE MODULES
# =============================================================================
# These modules provide ways to interact with the AI.

class APIServerModule(Module):
    """
    API server module - REST API interface.
    
    üìñ WHAT THIS DOES:
    Creates a REST API so other programs can talk to Enigma AI Engine.
    Great for building apps, bots, or integrating with other tools.
    
    üìê ENDPOINTS (when running):
    - POST /chat - Send a message, get a response
    - GET /health - Check if server is running
    - POST /generate - Generate text from prompt
    
    üîó WRAPS: enigma_engine/comms/api_server.py ‚Üí create_app()
    """

    INFO = ModuleInfo(
        id="api_server",
        name="API Server",
        description="REST API for remote access and integrations",
        category=ModuleCategory.INTERFACE,
        version="1.0.0",
        requires=["inference"],  # Need inference to generate responses
        provides=[
            "rest_api",       # Provides REST endpoints
            "remote_access"], # Enables remote use
        config_schema={
            "host": {"type": "string", "default": "127.0.0.1"},  # localhost by default (safe)
            "port": {"type": "int", "min": 1, "max": 65535, "default": 5000},
            "auth_enabled": {"type": "bool", "default": False},  # Enable API auth?
        },
    )

    def load(self) -> bool:
        """Load the API server factory."""
        from enigma_engine.comms.api_server import create_app
        self._app_factory = create_app
        return True


class NetworkModule(Module):
    """
    Network module - multi-device communication.
    
    üìñ WHAT THIS DOES:
    Lets multiple computers work together! You can:
    - Split inference across devices
    - Share models between machines
    - Run the brain on a server, UI on a tablet
    
    üìê NETWORK ROLES:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ standalone ‚îÇ Solo mode, no networking                             ‚îÇ
    ‚îÇ server     ‚îÇ Central hub that coordinates others                  ‚îÇ
    ‚îÇ client     ‚îÇ Connects to a server                                 ‚îÇ
    ‚îÇ peer       ‚îÇ Equal partner in a mesh network                      ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    
    üîó WRAPS: enigma_engine/comms/network.py ‚Üí ForgeNode class
    """

    INFO = ModuleInfo(
        id="network",
        name="Network System",
        description="Multi-device communication, distributed inference, model sharing",
        category=ModuleCategory.NETWORK,
        version="1.0.0",
        requires=[],
        optional=[
            "model",      # Can share models
            "inference"], # Can distribute inference
        provides=[
            "multi_device",           # Multiple computers can work together
            "distributed_inference",  # Split work across machines
            "model_sync"],            # Keep models in sync
        supports_distributed=True,
        config_schema={
            "role": {
                "type": "choice",
                "options": [
                        "standalone",  # No networking
                        "server",      # Central coordinator
                        "client",      # Connect to server
                        "peer"],       # Mesh network
                "default": "standalone"},
            "discovery": {
                "type": "bool",
                "default": True},  # Auto-discover other nodes?
        },
    )

    def load(self) -> bool:
        """Load the network node."""
        try:
            from enigma_engine.comms.network import ForgeNode
            self._instance = ForgeNode()
            return True
        except ImportError as e:
            logger.debug(f"ForgeNode not available (optional): {e}")
            return True  # Optional module
        except Exception as e:
            logger.warning(f"ForgeNode failed to initialize: {e}")
            return False


class TunnelModule(Module):
    """
    Tunnel module - expose server to internet via ngrok/localtunnel.
    
    üìñ WHAT THIS DOES:
    Creates secure tunnels to expose Enigma AI Engine to the internet using services
    like ngrok, localtunnel, or bore. Perfect for:
    - Remote access from anywhere
    - Mobile app connections
    - Demos and presentations
    - Team collaboration
    
    üìê TUNNEL PROVIDERS:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ ngrok        ‚îÇ Most reliable, requires account (free tier)        ‚îÇ
    ‚îÇ localtunnel  ‚îÇ Simple, no account needed (less stable)            ‚îÇ
    ‚îÇ bore         ‚îÇ Rust-based, fast, no account                       ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    
    üîó WRAPS: enigma_engine/comms/tunnel_manager.py ‚Üí TunnelManager class
    """

    INFO = ModuleInfo(
        id="tunnel",
        name="Tunnel Manager",
        description="Expose server to internet via ngrok/localtunnel/bore for remote access",
        category=ModuleCategory.NETWORK,
        version="1.0.0",
        requires=[],  # Can work standalone
        optional=[
            "api_server",  # Usually tunnel the API
            "web_server"], # Or tunnel the web UI
        provides=[
            "public_access",   # Exposes server publicly
            "remote_tunnel"],  # Creates secure tunnel
        config_schema={
            "provider": {
                "type": "choice",
                "options": [
                    "ngrok",       # Most reliable
                    "localtunnel", # No account needed
                    "bore"],       # Rust-based
                "default": "ngrok"},
            "auth_token": {
                "type": "string",
                "default": "",  # Required for ngrok
                "description": "Tunnel provider auth token (ngrok requires this)"},
            "region": {
                "type": "choice",
                "options": [
                    "us",  # United States
                    "eu",  # Europe
                    "ap",  # Asia Pacific
                    "au",  # Australia
                    "sa",  # South America
                    "jp",  # Japan
                    "in"], # India
                "default": "us",
                "description": "Server region (ngrok only)"},
            "subdomain": {
                "type": "string",
                "default": "",
                "description": "Custom subdomain (requires paid plan)"},
            "auto_start": {
                "type": "bool",
                "default": False,
                "description": "Auto-start tunnel when module loads"},
            "port": {
                "type": "int",
                "min": 1,
                "max": 65535,
                "default": 5000,
                "description": "Local port to expose"},
        },
    )

    def load(self) -> bool:
        """Load the tunnel manager."""
        try:
            from enigma_engine.comms.tunnel_manager import TunnelManager
            
            provider = self.config.get('provider', 'ngrok')
            auth_token = self.config.get('auth_token', None)
            region = self.config.get('region', None)
            subdomain = self.config.get('subdomain', None)
            
            # Create tunnel manager instance
            self._instance = TunnelManager(
                provider=provider,
                auth_token=auth_token,
                region=region,
                subdomain=subdomain,
                auto_reconnect=True
            )
            
            # Auto-start if configured
            if self.config.get('auto_start', False):
                port = self.config.get('port', 5000)
                tunnel_url = self._instance.start_tunnel(port)
                if tunnel_url:
                    logger.info(f"Tunnel auto-started: {tunnel_url}")
                else:
                    logger.warning("Failed to auto-start tunnel")
            
            return True
        except ImportError as e:
            logger.error(f"TunnelManager not available: {e}")
            return False
        except Exception as e:
            logger.error(f"TunnelManager failed to initialize: {e}")
            return False
    
    def unload(self) -> bool:
        """Unload and stop tunnel."""
        if self._instance is not None:
            try:
                self._instance.stop_tunnel()
            except Exception as e:
                logger.warning(f"Error stopping tunnel: {e}")
            finally:
                self._instance = None
        return True


class GUIModule(Module):
    """
    GUI module - graphical interface.
    
    üìñ WHAT THIS DOES:
    Provides the main graphical interface using PyQt5.
    Has tabs for chat, training, modules, and all the generation features.
    
    üìê THEMES:
    - dark: Easy on the eyes (default)
    - light: Bright mode
    - system: Match your OS settings
    
    üîó WRAPS: enigma_engine/gui/enhanced_window.py ‚Üí EnhancedMainWindow
    """

    INFO = ModuleInfo(
        id="gui",
        name="Graphical Interface",
        description="PyQt5-based GUI with chat, training, modules management",
        category=ModuleCategory.INTERFACE,
        version="2.0.0",
        requires=[],
        optional=[
            "model",        # For chat
            "tokenizer",    # For chat
            "inference",    # For text generation
            "training",     # For training tab
            "memory",       # For conversation history
            "voice_input",  # For voice commands
            "voice_output", # For spoken responses
            "vision",       # For vision tab
            "avatar"],      # For avatar display
        provides=[
            "graphical_interface",  # Main window
            "chat_ui",              # Chat interface
            "training_ui",          # Training controls
            "module_management"],   # Module toggle UI
        config_schema={
            "theme": {
                "type": "choice",
                "options": [
                        "dark",    # Dark mode
                        "light",   # Light mode
                        "system"], # Match OS
                "default": "dark"},
            "window_size": {
                "type": "choice",
                "options": [
                    "small",       # 800x600
                    "medium",      # 1200x800
                    "large",       # 1600x900
                    "fullscreen"], # Full screen
                "default": "medium"},
        },
    )

    def load(self) -> bool:
        """GUI is loaded on demand when window is created."""
        return True


# =============================================================================
# üé® GENERATION MODULES
# =============================================================================
# These modules generate content (images, code, video, audio, 3D, etc.)
# Each capability has LOCAL and API variants that CONFLICT with each other
# (you can only use one at a time - they both provide the same capability)

class GenerationModule(Module):
    """
    Base class for generation modules.
    
    üìñ PATTERN:
    All generation modules follow this pattern:
    1. Wrap an "addon" from the GUI tabs
    2. load() creates and loads the addon
    3. generate() calls the addon's generate method
    4. unload() cleans up the addon
    
    üìê LOCAL vs API:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ LOCAL modules: Run on your GPU, no internet needed, free          ‚îÇ
    ‚îÇ API modules: Run in the cloud, need internet + API key, paid      ‚îÇ
    ‚îÇ                                                                    ‚îÇ
    ‚îÇ ‚ö†Ô∏è LOCAL and API versions CONFLICT - can only load ONE at a time  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    """

    def __init__(self, manager, config=None):
        super().__init__(manager, config)
        self._addon = None  # The wrapped addon from GUI tabs

    def unload(self) -> bool:
        """Unload the wrapped addon."""
        if self._addon:
            self._addon.unload()
            self._addon = None
        return True

    def generate(self, prompt: str, **kwargs):
        """
        Generate content using the wrapped addon.
        
        Args:
            prompt: What to generate (text description)
            **kwargs: Additional options passed to the addon
            
        Returns:
            dict with 'success' and either 'result' or 'error'
        """
        if not self._addon or not self._addon.is_loaded:
            raise RuntimeError(f"Module not loaded")
        
        # Ensure prompt is a string (CLIP tokenizer requires str type)
        if prompt is not None:
            prompt = str(prompt).strip()
        if not prompt:
            return {"success": False, "error": "Prompt cannot be empty"}
        
        return self._addon.generate(prompt, **kwargs)

    def get_interface(self):
        """Return the addon for direct access."""
        return self._addon


# -----------------------------------------------------------------------------
# üñºÔ∏è IMAGE GENERATION
# -----------------------------------------------------------------------------

class ImageGenLocalModule(GenerationModule):
    """
    Local image generation with Stable Diffusion.
    
    üìñ WHAT THIS DOES:
    Generates images from text prompts using Stable Diffusion running
    locally on your GPU. No internet needed, completely free!
    
    üìê MODELS:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ sd-2.1      ‚îÇ Stable Diffusion 2.1 (6GB VRAM)                     ‚îÇ
    ‚îÇ sdxl        ‚îÇ Stable Diffusion XL (8GB VRAM, higher quality)      ‚îÇ
    ‚îÇ sdxl-turbo  ‚îÇ Fast version of SDXL (4 steps instead of 30)        ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    
    ‚ö†Ô∏è CONFLICTS: image_gen_api (can only use one image generator)
    üîó WRAPS: enigma_engine/gui/tabs/image_tab.py ‚Üí StableDiffusionLocal
    """

    INFO = ModuleInfo(
        id="image_gen_local",
        name="Image Generation (Local)",
        description="Generate images locally with Stable Diffusion. Requires GPU with 8GB+ VRAM.",
        category=ModuleCategory.GENERATION,
        version="1.0.0",
        requires=[],
        provides=["image_generation"],  # Both local and API provide this
        min_vram_mb=6000,  # Need at least 6GB VRAM
        requires_gpu=True,  # GPU required!
        config_schema={
            "model": {
                "type": "choice",
                "options": [
                    "sd-2.1",      # Smaller, works on 6GB
                    "sdxl",        # Better quality, needs 8GB
                    "sdxl-turbo"], # Fast but lower quality
                "default": "sd-2.1"},
            "steps": {
                "type": "int",
                "min": 1,
                "max": 100,
                "default": 30},  # More steps = better quality but slower
        },
    )

    def load(self) -> bool:
        """Load the Stable Diffusion model."""
        try:
            from enigma_engine.gui.tabs.image_tab import StableDiffusionLocal
            self._addon = StableDiffusionLocal()
            return self._addon.load()
        except Exception as e:
            logger.warning(f"Could not load local image gen: {e}")
            return False


class ImageGenAPIModule(GenerationModule):
    """
    Cloud image generation via APIs.
    
    üìñ WHAT THIS DOES:
    Generates images using cloud APIs (DALL-E, Replicate).
    Great if you don't have a GPU!
    
    üìê PROVIDERS:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ openai    ‚îÇ DALL-E 3 (best quality, $0.04/image)                  ‚îÇ
    ‚îÇ replicate ‚îÇ SDXL, Flux, etc (many models, usage-based pricing)    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    
    ‚ö†Ô∏è CONFLICTS: image_gen_local (can only use one image generator)
    üîó WRAPS: enigma_engine/gui/tabs/image_tab.py ‚Üí OpenAIImage, ReplicateImage
    """

    INFO = ModuleInfo(
        id="image_gen_api",
        name="Image Generation (Cloud)",
        description="Generate images via OpenAI DALL-E or Replicate (SDXL, Flux). Requires API key.",
        category=ModuleCategory.GENERATION,
        version="1.0.0",
        requires=[],
        provides=["image_generation"],  # Same capability as local
        is_cloud_service=True,  # Uses cloud APIs
        config_schema={
            "provider": {
                "type": "choice",
                "options": [
                    "openai",
                    "replicate"],
                "default": "openai"},
            "model": {
                "type": "string",
                "default": "dall-e-3"},
            "api_key": {
                "type": "secret",
                "default": ""},
        },
    )

    def load(self) -> bool:
        try:
            provider = self.config.get('provider', 'openai')
            if provider == 'openai':
                from enigma_engine.gui.tabs.image_tab import OpenAIImage
                self._addon = OpenAIImage(api_key=self.config.get('api_key'))
            else:
                from enigma_engine.gui.tabs.image_tab import ReplicateImage
                self._addon = ReplicateImage(api_key=self.config.get('api_key'))
            if self._addon.load():
                return True
        except Exception as e:
            logger.warning(f"Could not load cloud image gen: {e}")
        
        # Fall back to builtin image generator
        try:
            from enigma_engine.builtin import BuiltinImageGen
            self._addon = BuiltinImageGen()
            if self._addon.load():
                logger.info("Using built-in image generator as fallback")
                return True
        except Exception as e:
            logger.debug(f"Built-in image generator fallback failed: {e}")
        
        return True  # Module loads, just won't generate


# -----------------------------------------------------------------------------
# üíª CODE GENERATION
# -----------------------------------------------------------------------------

class CodeGenLocalModule(GenerationModule):
    """
    Local code generation using Forge model.
    
    üìñ WHAT THIS DOES:
    Generates code using your locally trained Forge model.
    Completely free and private - your code never leaves your machine!
    
    üìê TIP: Lower temperature (0.1-0.3) gives more predictable code
    
    üîó WRAPS: enigma_engine/gui/tabs/code_tab.py ‚Üí ForgeCode
    """

    INFO = ModuleInfo(
        id="code_gen_local",
        name="Code Generation (Local)",
        description="Generate code using your trained Forge model. Free and private.",
        category=ModuleCategory.GENERATION,
        version="1.0.0",
        requires=["model", "tokenizer", "inference"],  # Needs the full local stack
        provides=["code_generation"],
        config_schema={
            "model_name": {"type": "string", "default": None},
            "temperature": {"type": "float", "min": 0.1, "max": 1.5, "default": 0.3},
        },
    )

    def load(self) -> bool:
        """Load the local code generation model."""
        try:
            from enigma_engine.gui.tabs.code_tab import ForgeCode
            self._addon = ForgeCode(model_name=self.config.get('model_name') or 'small_enigma_engine')
            return self._addon.load()
        except Exception as e:
            logger.warning(f"Could not load local code gen: {e}")
            return False


class CodeGenAPIModule(GenerationModule):
    """
    Cloud code generation via OpenAI.
    
    üìñ WHAT THIS DOES:
    Uses OpenAI's GPT-4 to generate code. Very high quality,
    but requires an API key and costs money per request.
    
    üìê MODELS:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ gpt-4        ‚îÇ Best quality, most expensive                       ‚îÇ
    ‚îÇ gpt-4-turbo  ‚îÇ Faster, still very good                            ‚îÇ
    ‚îÇ gpt-3.5-turbo‚îÇ Cheapest, good for simple code                     ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    
    üîó WRAPS: enigma_engine/gui/tabs/code_tab.py ‚Üí OpenAICode
    """

    INFO = ModuleInfo(
        id="code_gen_api",
        name="Code Generation (Cloud)",
        description="Generate code via OpenAI GPT-4. High quality, requires API key.",
        category=ModuleCategory.GENERATION,
        version="1.0.0",
        requires=[],
        provides=["code_generation"],
        is_cloud_service=True,
        config_schema={
            "model": {
                "type": "choice",
                "options": [
                    "gpt-4",         # Best
                    "gpt-4-turbo",   # Fast + good
                    "gpt-3.5-turbo"],# Cheap
                "default": "gpt-4"},
            "api_key": {
                "type": "secret",
                "default": ""},
        },
    )

    def load(self) -> bool:
        """Load the OpenAI code generation client or fallback."""
        try:
            from enigma_engine.gui.tabs.code_tab import OpenAICode
            self._addon = OpenAICode(
                api_key=self.config.get('api_key'),
                model=self.config.get(
                    'model',
                    'gpt-4'))
            if self._addon.load():
                return True
        except Exception as e:
            logger.warning(f"Could not load cloud code gen: {e}")
        
        # Fall back to builtin code generator
        try:
            from enigma_engine.builtin import BuiltinCodeGen
            self._addon = BuiltinCodeGen()
            if self._addon.load():
                logger.info("Using built-in code generator as fallback")
                return True
        except Exception:
            pass
        
        return True  # Module loads, code gen may be limited


# -----------------------------------------------------------------------------
# üé¨ VIDEO GENERATION
# -----------------------------------------------------------------------------

class VideoGenLocalModule(GenerationModule):
    """
    Local video generation with AnimateDiff.
    
    üìñ WHAT THIS DOES:
    Generates short video clips from text prompts using AnimateDiff.
    Needs a powerful GPU (12GB+ VRAM recommended).
    
    üìê SETTINGS:
    - fps: Frames per second (4-30)
    - duration: Length in seconds (1-10)
    
    ‚ö†Ô∏è RESOURCE INTENSIVE: Uses lots of VRAM and takes time!
    
    üîó WRAPS: enigma_engine/gui/tabs/video_tab.py ‚Üí LocalVideo
    """

    INFO = ModuleInfo(
        id="video_gen_local",
        name="Video Generation (Local)",
        description="Generate videos locally with AnimateDiff. Requires powerful GPU.",
        category=ModuleCategory.GENERATION,
        version="1.0.0",
        requires=[],
        provides=["video_generation"],
        min_vram_mb=12000,  # Need 12GB VRAM minimum
        requires_gpu=True,
        config_schema={
            "fps": {"type": "int", "min": 4, "max": 30, "default": 8},
            "duration": {"type": "float", "min": 1, "max": 10, "default": 4},
        },
    )

    def load(self) -> bool:
        """Load the AnimateDiff video generator."""
        try:
            from enigma_engine.gui.tabs.video_tab import LocalVideo
            self._addon = LocalVideo()
            return self._addon.load()
        except Exception as e:
            logger.warning(f"Could not load local video gen: {e}")
            return False


class VideoGenAPIModule(GenerationModule):
    """
    Cloud video generation via Replicate.
    
    üìñ WHAT THIS DOES:
    Generates videos using cloud APIs (Zeroscope, etc).
    Good option if you don't have a powerful GPU.
    
    üîó WRAPS: enigma_engine/gui/tabs/video_tab.py ‚Üí ReplicateVideo
    """

    INFO = ModuleInfo(
        id="video_gen_api",
        name="Video Generation (Cloud)",
        description="Generate videos via Replicate (Zeroscope, etc). Requires API key.",
        category=ModuleCategory.GENERATION,
        version="1.0.0",
        requires=[],
        provides=["video_generation"],
        is_cloud_service=True,
        config_schema={
            "model": {"type": "string", "default": "zeroscope"},
            "api_key": {"type": "secret", "default": ""},
        },
    )

    def load(self) -> bool:
        """Load the Replicate video client or fallback."""
        try:
            from enigma_engine.gui.tabs.video_tab import ReplicateVideo
            self._addon = ReplicateVideo(api_key=self.config.get('api_key'))
            if self._addon.load():
                return True
        except Exception as e:
            logger.warning(f"Could not load cloud video gen: {e}")
        
        # Fall back to builtin video generator (animated GIF)
        try:
            from enigma_engine.builtin import BuiltinVideoGen
            self._addon = BuiltinVideoGen()
            if self._addon.load():
                logger.info("Using built-in video generator as fallback")
                return True
        except Exception:
            pass
        
        return True  # Module loads, video gen may be limited


# -----------------------------------------------------------------------------
# üîä AUDIO GENERATION
# -----------------------------------------------------------------------------

class AudioGenLocalModule(GenerationModule):
    """
    Local audio/TTS generation.
    
    üìñ WHAT THIS DOES:
    Converts text to spoken audio using local engines.
    Works offline, completely free!
    
    üìê ENGINES:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ pyttsx3  ‚îÇ Offline, robotic voice, very fast                      ‚îÇ
    ‚îÇ edge-tts ‚îÇ Microsoft Edge voices (needs internet for first use)   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    
    üîó WRAPS: enigma_engine/gui/tabs/audio_tab.py ‚Üí LocalTTS
    """

    INFO = ModuleInfo(
        id="audio_gen_local",
        name="Audio/TTS (Local)",
        description="Local text-to-speech and audio generation. Free, works offline.",
        category=ModuleCategory.GENERATION,
        version="1.0.0",
        requires=[],
        provides=["audio_generation", "text_to_speech"],
        config_schema={
            "engine": {"type": "choice", "options": ["pyttsx3", "edge-tts"], "default": "pyttsx3"},
            "voice": {"type": "string", "default": "default"},
        },
    )

    def load(self) -> bool:
        try:
            from enigma_engine.gui.tabs.audio_tab import LocalTTS
            self._addon = LocalTTS()
            return self._addon.load()
        except Exception as e:
            logger.warning(f"Could not load local audio gen: {e}")
            return False


class AudioGenAPIModule(GenerationModule):
    """
    Cloud audio generation via ElevenLabs/Replicate.
    
    üìñ WHAT THIS DOES:
    Premium text-to-speech with natural voices (ElevenLabs)
    or AI music generation (MusicGen via Replicate).
    
    üìê PROVIDERS:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ elevenlabs ‚îÇ Best TTS voices (natural, expressive)                ‚îÇ
    ‚îÇ replicate  ‚îÇ MusicGen and other audio models                      ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    
    üîó WRAPS: enigma_engine/gui/tabs/audio_tab.py ‚Üí ElevenLabsTTS, ReplicateAudio
    """

    INFO = ModuleInfo(
        id="audio_gen_api",
        name="Audio/TTS (Cloud)",
        description="Premium TTS via ElevenLabs or music via MusicGen. Requires API key.",
        category=ModuleCategory.GENERATION,
        version="1.0.0",
        requires=[],
        provides=[
            "audio_generation",    # Can make sounds
            "text_to_speech",      # Can speak text
            "music_generation"],   # Can make music
        is_cloud_service=True,
        config_schema={
            "provider": {
                "type": "choice",
                "options": [
                    "elevenlabs",  # Best TTS
                    "replicate"],  # MusicGen
                "default": "elevenlabs"},
            "api_key": {
                "type": "secret",
                        "default": ""},
            "voice": {
                "type": "string",
                "default": "Rachel"},  # ElevenLabs default voice
        },
    )

    def load(self) -> bool:
        """Load the audio generation provider or fallback."""
        try:
            provider = self.config.get('provider', 'elevenlabs')
            if provider == 'elevenlabs':
                from enigma_engine.gui.tabs.audio_tab import ElevenLabsTTS
                self._addon = ElevenLabsTTS(api_key=self.config.get('api_key'))
            else:
                from enigma_engine.gui.tabs.audio_tab import ReplicateAudio
                self._addon = ReplicateAudio(api_key=self.config.get('api_key'))
            if self._addon.load():
                return True
        except Exception as e:
            logger.warning(f"Could not load cloud audio gen: {e}")
        
        # Fall back to builtin TTS
        try:
            from enigma_engine.builtin import BuiltinTTS
            self._addon = BuiltinTTS()
            if self._addon.load():
                logger.info("Using built-in TTS as fallback")
                return True
        except Exception:
            pass
        
        return True  # Module loads, audio gen may be limited


# -----------------------------------------------------------------------------
# üß† EMBEDDING GENERATION
# -----------------------------------------------------------------------------

class EmbeddingLocalModule(GenerationModule):
    """
    Local embedding generation with sentence-transformers.
    
    üìñ WHAT THIS DOES:
    Converts text into vectors (numbers) that capture meaning.
    This enables semantic search - finding similar text even if
    the exact words are different.
    
    üìê MODELS:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ all-MiniLM-L6-v2    ‚îÇ Fast, good quality, 22M params             ‚îÇ
    ‚îÇ all-mpnet-base-v2   ‚îÇ Better quality, 109M params                ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    
    üîó WRAPS: enigma_engine/gui/tabs/embeddings_tab.py ‚Üí LocalEmbedding
    """

    INFO = ModuleInfo(
        id="embedding_local",
        name="Embeddings (Local)",
        description="Generate vector embeddings locally for semantic search. Free and private.",
        category=ModuleCategory.MEMORY,
        version="1.0.0",
        requires=[],
        provides=[
            "embeddings",        # Can create vectors from text
            "semantic_search"],  # Can find similar text
        config_schema={
            "model": {
                "type": "choice",
                "options": [
                    "all-MiniLM-L6-v2",     # Fast and small
                    "all-mpnet-base-v2"],   # Better quality
                "default": "all-MiniLM-L6-v2"},
        },
    )

    def load(self) -> bool:
        """Load the sentence-transformers model."""
        try:
            from enigma_engine.gui.tabs.embeddings_tab import LocalEmbedding
            self._addon = LocalEmbedding(model_name=self.config.get('model', 'all-MiniLM-L6-v2'))
            return self._addon.load()
        except Exception as e:
            logger.warning(f"Could not load local embeddings: {e}")
            return False


class EmbeddingAPIModule(GenerationModule):
    """
    Cloud embeddings via OpenAI.
    
    üìñ WHAT THIS DOES:
    Uses OpenAI's embedding models to convert text to vectors.
    Higher quality than local models, but requires API key.
    
    üìê MODELS:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ text-embedding-3-small ‚îÇ Cheaper, 1536 dimensions                 ‚îÇ
    ‚îÇ text-embedding-3-large ‚îÇ Better quality, 3072 dimensions          ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    
    üîó WRAPS: enigma_engine/gui/tabs/embeddings_tab.py ‚Üí OpenAIEmbedding
    """

    INFO = ModuleInfo(
        id="embedding_api",
        name="Embeddings (Cloud)",
        description="High-quality embeddings via OpenAI API. Requires API key.",
        category=ModuleCategory.MEMORY,
        version="1.0.0",
        requires=[],
        provides=[
            "embeddings",
            "semantic_search"],
        is_cloud_service=True,
        config_schema={
            "model": {
                "type": "choice",
                "options": [
                    "text-embedding-3-small",  # Cheaper
                    "text-embedding-3-large"], # Better
                "default": "text-embedding-3-small"},
            "api_key": {
                "type": "secret",
                        "default": ""},
        },
    )

    def load(self) -> bool:
        """Load the OpenAI embedding client or fallback to local."""
        try:
            from enigma_engine.gui.tabs.embeddings_tab import OpenAIEmbedding
            self._addon = OpenAIEmbedding(
                api_key=self.config.get('api_key'),
                model=self.config.get('model'))
            if self._addon.load():
                return True
        except Exception as e:
            logger.warning(f"Could not load cloud embeddings: {e}")
        
        # Fall back to local embeddings
        try:
            from enigma_engine.builtin import BuiltinEmbeddings
            self._addon = BuiltinEmbeddings()
            if self._addon.load():
                logger.info("Using built-in embeddings as fallback")
                return True
        except Exception as e2:
            logger.warning(f"Builtin embeddings also failed: {e2}")
        
        return True  # Module loads even without working embeddings


# -----------------------------------------------------------------------------
# üéÆ 3D GENERATION
# -----------------------------------------------------------------------------

class ThreeDGenLocalModule(GenerationModule):
    """
    Local 3D model generation with Shap-E or Point-E.
    
    üìñ WHAT THIS DOES:
    Generates 3D models from text descriptions or images.
    You can export to common formats like .obj, .ply, .glb.
    
    üìê MODELS:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ shap-e  ‚îÇ OpenAI's text-to-3D model (better quality)              ‚îÇ
    ‚îÇ point-e ‚îÇ OpenAI's point cloud model (faster)                     ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    
    üìê KEY PARAMETERS:
    - guidance_scale: How closely to follow the prompt (1-20)
    - num_inference_steps: More steps = better quality (10-100)
    
    üîó WRAPS: enigma_engine/gui/tabs/threed_tab.py ‚Üí Local3DGen
    """

    INFO = ModuleInfo(
        id="threed_gen_local",
        name="3D Generation (Local)",
        description="Generate 3D models from text/images locally. Requires GPU.",
        category=ModuleCategory.GENERATION,
        version="1.0.0",
        requires=[],
        provides=["3d_generation", "mesh_generation"],
        conflicts=["threed_gen_api"],  # Can only use one 3D generator
        min_vram_mb=4000,  # Need 4GB VRAM
        requires_gpu=True,
        config_schema={
            "model": {
                "type": "choice",
                "options": ["shap-e", "point-e"],
                "default": "shap-e"
            },
            "guidance_scale": {
                "type": "float",
                "min": 1.0,
                "max": 20.0,
                "default": 15.0  # Higher = follows prompt more closely
            },
            "num_inference_steps": {
                "type": "int",
                "min": 10,
                "max": 100,
                "default": 64  # More steps = better but slower
            },
        },
    )

    def load(self) -> bool:
        """Load the 3D generation model."""
        try:
            from enigma_engine.gui.tabs.threed_tab import Local3DGen
            self._addon = Local3DGen(
                model=self.config.get('model', 'shap-e')
            )
            return self._addon.load()
        except Exception as e:
            logger.warning(f"Could not load local 3D gen: {e}")
            return False


class ThreeDGenAPIModule(GenerationModule):
    """
    Cloud 3D model generation via API services.
    
    üìñ WHAT THIS DOES:
    Generates 3D models using cloud APIs.
    Good option if you don't have a GPU.
    
    üîó WRAPS: enigma_engine/gui/tabs/threed_tab.py ‚Üí Cloud3DGen
    """

    INFO = ModuleInfo(
        id="threed_gen_api",
        name="3D Generation (Cloud)",
        description="Generate 3D models via cloud APIs (Replicate, etc). Requires API key.",
        category=ModuleCategory.GENERATION,
        version="1.0.0",
        requires=[],
        provides=["3d_generation", "mesh_generation"],
        conflicts=["threed_gen_local"],  # Can only use one
        is_cloud_service=True,
        config_schema={
            "service": {
                "type": "choice",
                "options": ["replicate"],
                "default": "replicate"
            },
            "api_key": {
                "type": "secret",
                "default": ""
            },
        },
    )

    def load(self) -> bool:
        """Load the cloud 3D generation client or fallback."""
        try:
            from enigma_engine.gui.tabs.threed_tab import Cloud3DGen
            self._addon = Cloud3DGen(
                api_key=self.config.get('api_key'),
                service=self.config.get('service', 'replicate')
            )
            if self._addon.load():
                return True
        except Exception as e:
            logger.warning(f"Could not load cloud 3D gen: {e}")
        
        # Fall back to builtin 3D generator
        try:
            from enigma_engine.builtin import Builtin3DGen
            self._addon = Builtin3DGen()
            if self._addon.load():
                logger.info("Using built-in 3D generator as fallback")
                return True
        except Exception:
            pass
        
        return True  # Module loads, 3D gen may be limited


# -----------------------------------------------------------------------------
# üéØ MOTION TRACKING
# -----------------------------------------------------------------------------

class MotionTrackingModule(Module):
    """
    Motion tracking module for user mimicry.
    
    üìñ WHAT THIS DOES:
    Uses your webcam to track body pose, hands, and face.
    The avatar can then mimic your movements in real-time!
    
    üìê TRACKING MODES:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ pose     ‚îÇ Body pose only (33 landmarks)                          ‚îÇ
    ‚îÇ hands    ‚îÇ Hand tracking (21 landmarks per hand)                  ‚îÇ
    ‚îÇ face     ‚îÇ Facial mesh (468 landmarks)                            ‚îÇ
    ‚îÇ holistic ‚îÇ Everything combined (recommended)                      ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    
    üìê MODEL COMPLEXITY:
    - 0: Lite (fastest, least accurate)
    - 1: Full (balanced)
    - 2: Heavy (most accurate, slowest)
    
    üîó WRAPS: enigma_engine/tools/motion_tracking.py ‚Üí MotionTracker
    """

    INFO = ModuleInfo(
        id="motion_tracking",
        name="Motion Tracking",
        description="Real-time motion tracking with MediaPipe for gesture mimicry",
        category=ModuleCategory.PERCEPTION,
        version="1.0.0",
        requires=[],
        optional=["avatar"],  # Can control avatar with movements
        provides=[
            "motion_tracking",       # Track body movements
            "gesture_recognition",   # Recognize gestures
            "pose_estimation"],      # Estimate body pose
        config_schema={
            "camera_id": {
                "type": "int",
                "min": 0,
                "max": 10,
                "default": 0  # Default webcam
            },
            "tracking_mode": {
                "type": "choice",
                "options": ["pose", "hands", "face", "holistic"],
                "default": "holistic"  # Track everything
            },
            "model_complexity": {
                "type": "int",
                "min": 0,
                "max": 2,
                "default": 1  # Balanced
            },
        },
    )

    def load(self) -> bool:
        """Load the MediaPipe motion tracker or enable deferred loading."""
        try:
            from enigma_engine.tools.motion_tracking import MotionTracker
            self._instance = MotionTracker(
                camera_id=self.config.get('camera_id', 0),
                tracking_mode=self.config.get('tracking_mode', 'holistic')
            )
            return True
        except ImportError:
            logger.info("MediaPipe not available - motion tracking disabled. Install: pip install mediapipe")
            return True  # Module loads, shows install instructions in UI
        except Exception as e:
            logger.warning(f"Could not load motion tracking: {e}")
            return True  # Still load module, UI will show error


# -----------------------------------------------------------------------------
# CAMERA MODULE
# -----------------------------------------------------------------------------

class CameraModule(Module):
    """
    Camera module for webcam capture and analysis.
    
    Provides webcam access for capturing images, recording video,
    and analyzing camera feed with AI vision.
    """

    INFO = ModuleInfo(
        id="camera",
        name="Camera",
        description="Webcam capture, recording, and AI-powered visual analysis",
        category=ModuleCategory.PERCEPTION,
        version="1.0.0",
        requires=[],
        optional=["vision"],  # For AI analysis
        provides=["camera_capture", "video_recording"],
        config_schema={
            "camera_id": {
                "type": "int",
                "min": 0,
                "max": 10,
                "default": 0
            },
        },
    )

    def load(self) -> bool:
        """Load camera module - verifies OpenCV is available."""
        try:
            import cv2
            self._cv2 = cv2
            return True
        except ImportError:
            logger.warning("Camera module requires opencv-python: pip install opencv-python")
            return True  # Still allow load, tab shows install instructions


# -----------------------------------------------------------------------------
# GIF GENERATION MODULE
# -----------------------------------------------------------------------------

class GIFGenModule(Module):
    """
    GIF generation module.
    
    Create animated GIFs from images, videos, or AI generation.
    """

    INFO = ModuleInfo(
        id="gif_gen",
        name="GIF Generation",
        description="Create animated GIFs from images, videos, or AI generation",
        category=ModuleCategory.GENERATION,
        version="1.0.0",
        requires=[],
        optional=["image_gen_local", "image_gen_api"],
        provides=["gif_generation"],
        config_schema={},
    )

    def load(self) -> bool:
        """Load GIF generation module - verifies PIL is available."""
        try:
            from PIL import Image
            self._pil = Image
            return True
        except ImportError:
            logger.warning("GIF module requires Pillow: pip install Pillow")
            return True


# -----------------------------------------------------------------------------
# VOICE CLONE MODULE
# -----------------------------------------------------------------------------

class VoiceCloneModule(Module):
    """
    Voice cloning module.
    
    Clone voices from audio samples for TTS synthesis.
    """

    INFO = ModuleInfo(
        id="voice_clone",
        name="Voice Cloning",
        description="Clone voices from audio samples for personalized TTS",
        category=ModuleCategory.OUTPUT,
        version="1.0.0",
        requires=[],
        optional=["voice_output"],
        provides=["voice_cloning"],
        config_schema={},
    )

    def load(self) -> bool:
        """Load voice cloning module - verifies TTS backend."""
        try:
            return True
        except Exception as e:
            logger.warning(f"Voice clone module load: {e}")
            return True


# -----------------------------------------------------------------------------
# NOTES MODULE
# -----------------------------------------------------------------------------

class NotesModule(Module):
    """
    Notes module for saving and organizing thoughts.
    
    Provides persistent notes that the AI can access and reference.
    """

    INFO = ModuleInfo(
        id="notes",
        name="Notes",
        description="Save and organize notes that persist across sessions",
        category=ModuleCategory.MEMORY,
        version="1.0.0",
        requires=[],
        provides=["notes_storage"],
        config_schema={},
    )

    def load(self) -> bool:
        """Load notes module - initializes notes manager."""
        try:
            from enigma_engine.gui.tabs.notes_tab import NOTES_DIR, NotesManager
            NOTES_DIR.mkdir(parents=True, exist_ok=True)
            self._manager = NotesManager()
            return True
        except Exception as e:
            logger.warning(f"Notes module load: {e}")
            return True


# -----------------------------------------------------------------------------
# SESSIONS MODULE
# -----------------------------------------------------------------------------

class SessionsModule(Module):
    """
    Sessions module for conversation management.
    
    Save, load, and manage multiple conversation sessions.
    """

    INFO = ModuleInfo(
        id="sessions",
        name="Sessions",
        description="Manage multiple conversation sessions with save/load functionality",
        category=ModuleCategory.MEMORY,
        version="1.0.0",
        requires=["memory"],
        provides=["session_management"],
        config_schema={},
    )

    def load(self) -> bool:
        """Load sessions module."""
        return True


# -----------------------------------------------------------------------------
# SCHEDULER MODULE
# -----------------------------------------------------------------------------

class SchedulerModule(Module):
    """
    Scheduler module for timed tasks.
    
    Schedule AI tasks, reminders, and automated actions.
    """

    INFO = ModuleInfo(
        id="scheduler",
        name="Scheduler",
        description="Schedule timed tasks, reminders, and automated AI actions",
        category=ModuleCategory.TOOLS,
        version="1.0.0",
        requires=[],
        optional=["inference", "chat_api"],
        provides=["task_scheduling"],
        config_schema={},
    )

    def load(self) -> bool:
        """Load scheduler module - initializes task scheduler."""
        try:
            from enigma_engine.gui.tabs.scheduler_tab import SCHEDULER_FILE
            SCHEDULER_FILE.parent.mkdir(parents=True, exist_ok=True)
            return True
        except Exception as e:
            logger.warning(f"Scheduler module load: {e}")
            return True


# -----------------------------------------------------------------------------
# PERSONALITY MODULE
# -----------------------------------------------------------------------------

class PersonalityModule(Module):
    """
    Personality module for AI behavior customization.
    
    Configure and customize the AI's personality, tone, and behavior.
    """

    INFO = ModuleInfo(
        id="personality",
        name="Personality",
        description="Customize AI personality, tone, communication style",
        category=ModuleCategory.CORE,
        version="1.0.0",
        requires=[],
        provides=["personality_config"],
        config_schema={},
    )

    def load(self) -> bool:
        """Load personality module."""
        return True


# -----------------------------------------------------------------------------
# TERMINAL MODULE
# -----------------------------------------------------------------------------

class TerminalModule(Module):
    """
    Terminal module for command execution.
    
    Provides terminal access for running system commands (with safety limits).
    """

    INFO = ModuleInfo(
        id="terminal",
        name="Terminal",
        description="Execute system commands with AI assistance and safety limits",
        category=ModuleCategory.TOOLS,
        version="1.0.0",
        requires=[],
        provides=["command_execution"],
        config_schema={},
    )

    def load(self) -> bool:
        """Load terminal module - verifies subprocess access."""
        try:
            import subprocess
            self._subprocess = subprocess
            return True
        except Exception as e:
            logger.warning(f"Terminal module load: {e}")
            return True


# -----------------------------------------------------------------------------
# ANALYTICS MODULE
# -----------------------------------------------------------------------------

class AnalyticsModule(Module):
    """
    Analytics module for usage statistics and insights.
    
    Tracks usage patterns, performance metrics, and provides insights.
    """

    INFO = ModuleInfo(
        id="analytics",
        name="Analytics",
        description="Usage statistics, performance metrics, and AI insights",
        category=ModuleCategory.INTERFACE,
        version="1.0.0",
        requires=[],
        provides=["analytics", "usage_stats"],
        config_schema={},
    )

    def load(self) -> bool:
        return True


# -----------------------------------------------------------------------------
# DASHBOARD MODULE
# -----------------------------------------------------------------------------

class DashboardModule(Module):
    """
    Dashboard module for system overview.
    
    Provides a unified view of AI status, resources, and quick actions.
    """

    INFO = ModuleInfo(
        id="dashboard",
        name="Dashboard",
        description="System overview with status, resources, and quick actions",
        category=ModuleCategory.INTERFACE,
        version="1.0.0",
        requires=[],
        provides=["dashboard_ui"],
        config_schema={},
    )

    def load(self) -> bool:
        return True


# -----------------------------------------------------------------------------
# EXAMPLES MODULE
# -----------------------------------------------------------------------------

class ExamplesModule(Module):
    """
    Examples module for prompt templates and tutorials.
    
    Provides example prompts, templates, and usage guides.
    """

    INFO = ModuleInfo(
        id="examples",
        name="Examples",
        description="Prompt templates, examples, and usage tutorials",
        category=ModuleCategory.INTERFACE,
        version="1.0.0",
        requires=[],
        provides=["examples", "templates"],
        config_schema={},
    )

    def load(self) -> bool:
        return True


# -----------------------------------------------------------------------------
# INSTRUCTIONS MODULE
# -----------------------------------------------------------------------------

class InstructionsModule(Module):
    """
    Instructions module for system prompts.
    
    Manage system instructions that define AI behavior.
    """

    INFO = ModuleInfo(
        id="instructions",
        name="Instructions",
        description="System prompts and behavior instructions for the AI",
        category=ModuleCategory.CORE,
        version="1.0.0",
        requires=[],
        provides=["system_prompts"],
        config_schema={},
    )

    def load(self) -> bool:
        return True


# -----------------------------------------------------------------------------
# LOGS MODULE
# -----------------------------------------------------------------------------

class LogsModule(Module):
    """
    Logs module for viewing system logs.
    
    View and search application logs for debugging.
    """

    INFO = ModuleInfo(
        id="logs",
        name="Logs",
        description="View and search system logs for debugging",
        category=ModuleCategory.INTERFACE,
        version="1.0.0",
        requires=[],
        provides=["log_viewer"],
        config_schema={},
    )

    def load(self) -> bool:
        return True


# -----------------------------------------------------------------------------
# MODEL ROUTER MODULE (different from tool_router)
# -----------------------------------------------------------------------------

class ModelRouterModule(Module):
    """
    Model router for specialized model assignment.
    
    Route different tasks to specialized AI models.
    """

    INFO = ModuleInfo(
        id="model_router",
        name="Model Router",
        description="Route tasks to specialized models (code, vision, etc.)",
        category=ModuleCategory.CORE,
        version="1.0.0",
        requires=[],
        optional=["model", "chat_api"],
        provides=["model_routing"],
        config_schema={},
    )

    def load(self) -> bool:
        """Load model router module - initializes routing config."""
        try:
            from enigma_engine.core.tool_router import get_router
            self._router = get_router()
            return True
        except Exception as e:
            logger.warning(f"Model router module load: {e}")
            return True


# -----------------------------------------------------------------------------
# SCALING MODULE
# -----------------------------------------------------------------------------

class ScalingModule(Module):
    """
    Scaling module for model size management.
    
    Grow or shrink models, manage model variants.
    """

    INFO = ModuleInfo(
        id="scaling",
        name="Model Scaling",
        description="Grow/shrink models, manage size variants",
        category=ModuleCategory.CORE,
        version="1.0.0",
        requires=["model"],
        provides=["model_scaling"],
        config_schema={},
    )

    def load(self) -> bool:
        return True


# -----------------------------------------------------------------------------
# GAME AI MODULE
# -----------------------------------------------------------------------------

class GameAIModule(Module):
    """
    Game AI module for gaming assistance.
    
    AI features for gaming: strategy, tactics, overlay assistance.
    """

    INFO = ModuleInfo(
        id="game_ai",
        name="Game AI",
        description="Gaming AI assistant with strategy and tactical advice",
        category=ModuleCategory.TOOLS,
        version="1.0.0",
        requires=[],
        optional=["vision", "voice_output"],
        provides=["game_assistance"],
        config_schema={},
    )

    def load(self) -> bool:
        """Load game AI module - initializes game router."""
        try:
            from enigma_engine.tools.game_router import get_game_router
            self._router = get_game_router()
            return True
        except Exception as e:
            logger.warning(f"Game AI module load: {e}")
            return True


# -----------------------------------------------------------------------------
# ROBOT CONTROL MODULE
# -----------------------------------------------------------------------------

class RobotControlModule(Module):
    """
    Robot control module for hardware integration.
    
    Control robots, servos, and hardware via AI commands.
    """

    INFO = ModuleInfo(
        id="robot_control",
        name="Robot Control",
        description="Control robots and hardware with AI commands",
        category=ModuleCategory.TOOLS,
        version="1.0.0",
        requires=[],
        provides=["robot_control", "hardware_interface"],
        config_schema={},
    )

    def load(self) -> bool:
        """Load robot control module - initializes mode controller."""
        try:
            from enigma_engine.tools.robot_modes import get_mode_controller
            self._controller = get_mode_controller()
            return True
        except Exception as e:
            logger.warning(f"Robot control module load: {e}")
            return True


# -----------------------------------------------------------------------------
# WORKSPACE MODULE
# -----------------------------------------------------------------------------

class WorkspaceModule(Module):
    """
    Workspace module for project management.
    
    Manage AI projects, training data, and configurations.
    """

    INFO = ModuleInfo(
        id="workspace",
        name="Workspace",
        description="Project management for training data and configurations",
        category=ModuleCategory.INTERFACE,
        version="1.0.0",
        requires=[],
        provides=["workspace_management"],
        config_schema={},
    )

    def load(self) -> bool:
        return True


# -----------------------------------------------------------------------------
# HUGGINGFACE MODULE
# -----------------------------------------------------------------------------

class HuggingFaceModule(Module):
    """
    HuggingFace integration module.
    
    Load and use models from HuggingFace Hub.
    """

    INFO = ModuleInfo(
        id="huggingface",
        name="HuggingFace",
        description="Load and use models from HuggingFace Hub",
        category=ModuleCategory.CORE,
        version="1.0.0",
        requires=[],
        provides=["hf_models", "model_hub"],
        config_schema={},
    )

    def load(self) -> bool:
        """Load HuggingFace module - verifies transformers is available."""
        try:
            from enigma_engine.core.huggingface_loader import load_huggingface_model
            self._loader = load_huggingface_model
            return True
        except Exception as e:
            logger.warning(f"HuggingFace module requires transformers: pip install transformers")
            return True


# =============================================================================
# üìö MODULE REGISTRY
# =============================================================================
# This is the master catalog of all available modules.
# The ModuleManager uses this to know what modules exist.
#
# üìê STRUCTURE:
# ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
# ‚îÇ  'module_id': ModuleClass,                                             ‚îÇ
# ‚îÇ                                                                        ‚îÇ
# ‚îÇ  Example:                                                              ‚îÇ
# ‚îÇ  'image_gen_local': ImageGenLocalModule,  # Local Stable Diffusion    ‚îÇ
# ‚îÇ  'image_gen_api': ImageGenAPIModule,      # Cloud DALL-E/Replicate    ‚îÇ
# ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
#
# üìê CATEGORIES:
# - Core: model, tokenizer, training, inference, chat_api, gguf_loader
# - Memory: memory, embedding_local, embedding_api
# - Perception: voice_input, vision, motion_tracking
# - Output: voice_output, avatar
# - Generation: image, code, video, audio, 3D (local and API variants)
# - Tools: web_tools, file_tools, tool_router
# - Network: api_server, network
# - Interface: gui

MODULE_REGISTRY: Dict[str, Type[Module]] = {
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # Core modules - The foundation of Enigma AI Engine
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    'model': ModelModule,           # The neural network brain
    'tokenizer': TokenizerModule,   # Text ‚Üî numbers converter
    'training': TrainingModule,     # Teaches the model
    'inference': InferenceModule,   # Generates text
    'chat_api': ChatAPIModule,      # Cloud chat (Ollama, GPT, Claude)
    'gguf_loader': GGUFLoaderModule,# Load llama.cpp models

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # Memory modules - Remember things
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    'memory': MemoryModule,                 # Conversation storage
    'embedding_local': EmbeddingLocalModule,# Local vector embeddings
    'embedding_api': EmbeddingAPIModule,    # Cloud embeddings
    'notes': NotesModule,                   # Persistent notes
    'sessions': SessionsModule,             # Conversation sessions

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # Perception modules - See and hear
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    'voice_input': VoiceInputModule,        # Speech-to-text
    'vision': VisionModule,                 # Image/screen analysis
    'motion_tracking': MotionTrackingModule,# Body/hand/face tracking
    'camera': CameraModule,                 # Webcam capture

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # Output modules - Speak and show
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    'voice_output': VoiceOutputModule,  # Text-to-speech
    'avatar': AvatarModule,             # Visual character
    'voice_clone': VoiceCloneModule,    # Voice cloning

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # Generation modules - Create content
    # (LOCAL and API versions conflict - only use one at a time!)
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    'image_gen_local': ImageGenLocalModule,   # Stable Diffusion
    'image_gen_api': ImageGenAPIModule,       # DALL-E / Replicate
    'code_gen_local': CodeGenLocalModule,     # Local Forge code
    'code_gen_api': CodeGenAPIModule,         # GPT-4 code
    'video_gen_local': VideoGenLocalModule,   # AnimateDiff
    'video_gen_api': VideoGenAPIModule,       # Replicate video
    'audio_gen_local': AudioGenLocalModule,   # pyttsx3 / edge-tts
    'audio_gen_api': AudioGenAPIModule,       # ElevenLabs / MusicGen
    'threed_gen_local': ThreeDGenLocalModule, # Shap-E / Point-E
    'threed_gen_api': ThreeDGenAPIModule,     # Cloud 3D
    'gif_gen': GIFGenModule,                  # GIF creation

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # Tool modules - Interact with the world
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    'web_tools': WebToolsModule,        # Web search and fetch
    'file_tools': FileToolsModule,      # File read/write
    'tool_router': ToolRouterModule,    # Route to specialized models
    'scheduler': SchedulerModule,       # Timed tasks/reminders
    'terminal': TerminalModule,         # Command execution
    'game_ai': GameAIModule,            # Gaming AI assistant
    'robot_control': RobotControlModule,# Robot/hardware control

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # Network modules - Connect devices
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    'api_server': APIServerModule,  # REST API server
    'network': NetworkModule,       # Multi-device networking
    'tunnel': TunnelModule,         # Public internet tunneling

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # Interface modules - User interaction
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    'gui': GUIModule,               # PyQt5 graphical interface
    'personality': PersonalityModule,  # AI personality customization
    'analytics': AnalyticsModule,   # Usage stats and insights
    'dashboard': DashboardModule,   # System overview
    'examples': ExamplesModule,     # Prompt templates
    'instructions': InstructionsModule,  # System prompts
    'logs': LogsModule,             # Log viewer
    'model_router': ModelRouterModule,   # Specialized model routing
    'scaling': ScalingModule,       # Model grow/shrink
    'workspace': WorkspaceModule,   # Project management
    'huggingface': HuggingFaceModule,    # HuggingFace Hub integration
}


# =============================================================================
# üîß HELPER FUNCTIONS
# =============================================================================
# These functions make it easy to work with the module registry.

def register_all(manager: ModuleManager):
    """
    Register all built-in modules with a manager.
    
    üìñ USAGE:
    ```python
    manager = ModuleManager()
    register_all(manager)  # Now manager knows about all modules
    ```
    """
    for module_class in MODULE_REGISTRY.values():
        manager.register(module_class)


@lru_cache(maxsize=128)
def get_module(module_id: str) -> Optional[Type[Module]]:
    """
    Get a module class by ID.
    
    üìñ EXAMPLE:
    ```python
    ImageClass = get_module('image_gen_local')  # Returns ImageGenLocalModule
    ```
    """
    return MODULE_REGISTRY.get(module_id)


@lru_cache(maxsize=32)
def list_modules() -> List[ModuleInfo]:
    """
    List all available modules.
    
    üìñ RETURNS:
    List of ModuleInfo objects with id, name, description, etc.
    """
    return [cls.get_info() for cls in MODULE_REGISTRY.values()]


def list_by_category(category: ModuleCategory) -> List[ModuleInfo]:
    """
    List modules by category.
    
    üìñ EXAMPLE:
    ```python
    gen_modules = list_by_category(ModuleCategory.GENERATION)
    ```
    """
    return [
        cls.get_info() for cls in MODULE_REGISTRY.values()
        if cls.get_info().category == category
    ]


def list_local_modules() -> List[ModuleInfo]:
    """
    List modules that run 100% locally (no cloud/internet required).
    Great for privacy-conscious users or offline use!
    """
    return [
        cls.get_info() for cls in MODULE_REGISTRY.values()
        if not cls.get_info().is_cloud_service
    ]


def list_cloud_modules() -> List[ModuleInfo]:
    """List modules that require cloud services and API keys."""
    return [
        cls.get_info() for cls in MODULE_REGISTRY.values()
        if cls.get_info().is_cloud_service
    ]
